{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff20d5f2",
   "metadata": {},
   "source": [
    "## basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019c297",
   "metadata": {},
   "source": [
    "- policy function: $\\pi(a|s)$ is a PDF\n",
    "    - $\\sum_{a}\\pi(a|s)=1$\n",
    "- Policy Network: $\\pi(a|s;\\mathbf \\theta)$\n",
    "    - $\\bf\\theta$ 表示神经网络的待训练参数\n",
    "    - $\\sum_{a}\\pi(a|s;\\theta)=1$（last layer of nn，称之为 logits，接一个 softmax 的变换）\n",
    "- 状态价值函数（state value function），对其做函数近似（function approximation）；\n",
    "\n",
    "    - **Discounted reward**\n",
    "\n",
    "        $$\n",
    "        U_t=R_t+\\gamma R_{t+1} + \\gamma^2R_{t+2} + \\gamma^3R_{t+3}+\\cdots\n",
    "        $$\n",
    "\n",
    "        - $U_t$ 的精确计算依赖 $A_t,A_{t+1},A_{t+2}, \\cdots, $ 和 $S_t,S_{t+1},S_{t+2},\\cdots$\n",
    "            - 是未来所有奖励的加和\n",
    "        - $t$ 时刻，未来的 reward 为还未观测到的随机变量，\n",
    "            - 每个奖励 $R_t$ 的随机性都来自于前一个时刻的动作 $A_t$ 和 $S_t$\n",
    "            - 动作的随机性来自于策略函数 $\\pi(a|\\cdot)$，状态的随机性来自于状态转移函数 $p(s_t|s_{t-1})$\n",
    "                - 随机性意味着它们（$A_t,S_t$）都是随机变量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e23416",
   "metadata": {},
   "source": [
    "### action-value function 与 state-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20dd06f",
   "metadata": {},
   "source": [
    "- **action-value function**（通过条件期望来定义）\n",
    "    $$\n",
    "    Q_\\pi(s_t,a_t)=\\mathbb E[U_t|S_t=s_t,A_t=a_t]\n",
    "    $$\n",
    "    - $Q_\\pi(s_t,a_t)$ 用来评价在状态 $S_t=a_t$ 下选择动作 $a_t$ 的好坏程度\n",
    "- **state-value function**（是 $Q_\\pi$ 的期望，将 $Q_\\pi$ 中的动作 $A\\sim \\pi(\\cdot|s_t)$ 积分掉）\n",
    "\n",
    "    $$\n",
    "    V_\\pi(s_t)=\\mathbb E_A\\left[Q_\\pi(s_t,A)\\right]\n",
    "    $$\n",
    "    \n",
    "    - $V_\\pi(s_t)$ 越大，说明当前状态 $s_t$ 的胜算越大；\n",
    "    - 如果 $A\\sim \\pi(\\cdot|s_t)$ 是离散的，则有：\n",
    "    \n",
    "    $$\n",
    "    V_\\pi(s_t)=\\mathbb E_A\\left[Q_\\pi(s_t,A)\\right]=\\sum_a\\pi(a|s_t)Q_\\pi(s_t,a)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f0a0a",
   "metadata": {},
   "source": [
    "### 策略学习的目标函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59810a",
   "metadata": {},
   "source": [
    "- $S_t,A_t,S_{t+1},A_{t+1}, S_{t+2}, A_{t+2}$\n",
    "- $Q_\\pi(s_t,a_t)=\\mathbb E[U_t|S_t=s_t,A_t=a_t]$\n",
    "- $V_\\pi(s_t;\\theta)=\\mathbb E_{A_t\\sim \\pi(\\cdot|s_t;\\theta)}Q_\\pi(s_t,A_t)=\\sum_a\\pi(a|s;\\theta)Q_\\pi(s,a)$\n",
    "- 状态价值（state value）既依赖当前状态 $s_t$ 也依赖策略网络 $\\pi$ 的参数 $\\theta$\n",
    "    - 当前状态 $s_t$ 越好，$V_\\pi(s_t)$ 越大，回报 $U_t$ 的期望也就越大；\n",
    "    - 策略网络 $\\pi$ 越好，$V_\\pi(s_t)$ 也会越大；\n",
    "- 定义目标函数\n",
    "\n",
    "    $$\n",
    "    \\begin{split}\n",
    "    &J(\\theta)=\\mathbb E_S[V_\\pi(S)]\\\\\n",
    "    &\\max_{\\theta} J(\\theta)\n",
    "    \\end{split}\n",
    "    $$\n",
    "    \n",
    "    - 最大化目标，通过梯度上升来优化\n",
    "    \n",
    "    $$\n",
    "    \\theta\\leftarrow \\theta+\\eta \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9c3937",
   "metadata": {},
   "source": [
    "### 策略梯度（policy gradient）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b8629b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta\\leftarrow \\theta+\\eta \\frac{\\partial V(s;\\theta)}{\\partial \\theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5b25b",
   "metadata": {},
   "source": [
    "- 策略梯度不是真正的梯度，真正的梯度是 $\\frac{\\partial J(\\theta)}{\\partial \\theta}$\n",
    "- 而 state-value function $V(s;\\theta)$ 关于 $\\theta$ 的导数称之为 policy gradient\n",
    "    - 其实是一个随机梯度，随机性来源于 $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f827c7dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T14:55:27.589062Z",
     "start_time": "2023-08-23T14:55:27.585421Z"
    }
   },
   "source": [
    "### 策略梯度定理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d59a8b8",
   "metadata": {},
   "source": [
    "$$V_\\pi(s_t;\\theta)=\\mathbb E_{A_t\\sim \\pi(\\cdot|s_t;\\theta)}Q_\\pi(s_t,A_t)=\\sum_a\\pi(a|s;\\theta)Q_\\pi(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31756a75",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "&\n",
    "\\frac{\\partial V(s;\\theta)}{\\partial \\theta}\\approx \\sum_a \\frac{\\partial \\pi(a|s;\\theta)}{\\partial \\theta}Q_\\pi(s,a)\\\\\n",
    "&\n",
    "\\frac{\\partial V(s;\\theta)}{\\partial \\theta}\\approx\\mathbb E_{A\\sim \\pi(\\cdot|S;\\theta)}\\left[\\frac{\\partial \\ln_\\pi(A|S;\\theta)}{\\partial \\theta}\\cdot Q_\\pi(S,A)\\right]\n",
    "\\\\\n",
    "&\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta}\\approx\\mathbb E_S\\left[\\mathbb E_{A\\sim \\pi(\\cdot|S;\\theta)}\\left[\\frac{\\partial \\ln_\\pi(A|S;\\theta)}{\\partial \\theta}\\cdot Q_\\pi(S,A)\\right]\\right]\n",
    "\\end{split} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9390e033",
   "metadata": {},
   "source": [
    "#### 离散型计算示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e43037",
   "metadata": {},
   "source": [
    "- 采用第一个公式\n",
    "- 记 $f(a,\\theta)=\\frac{\\partial \\pi(a|s;\\theta)}{\\partial \\theta}Q_\\pi(s,a)$\n",
    "    - 然后枚举所有的离散型动作 $a$，计算 $\\frac{\\partial V(s;\\theta)}{\\partial \\theta}=\\sum_af(a,\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e776b1",
   "metadata": {},
   "source": [
    "#### 连续型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f800ba3d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial V(s;\\theta)}{\\partial \\theta}\\approx\\mathbb E_{A\\sim \\pi(\\cdot|S;\\theta)}\\left[\\frac{\\partial \\ln_\\pi(A|S;\\theta)}{\\partial \\theta}\\cdot Q_\\pi(S,A)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc66ae0",
   "metadata": {},
   "source": [
    "- $\\pi(\\cdot|S;\\theta)$ 是一个神经网络，难以直接求定积分\n",
    "- 只好做蒙特卡洛近似，将期望近似算出来\n",
    "    - sample $\\hat a\\sim \\pi(\\cdot|s;\\mathbf \\theta)$\n",
    "    - 计算 $g(\\hat a, \\theta)=\\frac{\\partial \\ln_\\pi(\\hat a|s;\\theta)}{\\partial \\theta}\\cdot Q_\\pi(s,\\hat a)$\n",
    "        - 基于 pytorch 等深度学习框架的自动求导机制；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b00e1",
   "metadata": {},
   "source": [
    "### 策略梯度 algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f5a21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T15:02:10.285375Z",
     "start_time": "2023-09-04T15:02:10.277514Z"
    }
   },
   "source": [
    "1. observe the state $s_t$\n",
    "2. sample $a_t \\sim \\pi(\\cdot|s_t;\\theta_t)$\n",
    "3. compute $q_t\\approx Q_\\pi(s_t,a_t)$ ？？\n",
    "4. differentiate policy network $d_{\\theta,t}=\\frac{\\partial \\log\\pi(a_t|s_t;\\theta)}{\\partial \\theta}\\big|_{\\theta=\\theta_t}$\n",
    "5. **policy gradient**: $g(a_t,\\theta_t)=q_td_{\\theta,t}$\n",
    "6. update policy network: $\\theta_{t+1}=\\theta_t+\\beta\\cdot g(a_t,\\theta_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43cac7a",
   "metadata": {},
   "source": [
    "#### 计算 $Q_\\pi(s_t,a_t)$ 的 REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4264f54c",
   "metadata": {},
   "source": [
    "- trajectory：$(s_1,a_1,r_1), (s_2, a_2, r_2),\\cdots, (s_T, a_T, r_T )$\n",
    "- compute discounted return $u_t=\\sum_{k=t}^T\\gamma^{k-t}r_k, \\forall t$\n",
    "- since $Q_\\pi(s_t,a_t)=E[U_t]$, use $u_t$ approximate $Q_\\pi(s_t,a_t)$\n",
    "- $q_t\\approx Q_\\pi(s_t,a_t)\\approx u_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96336295",
   "metadata": {},
   "source": [
    "## REINFOCE with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7810f49a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65c003ef",
   "metadata": {},
   "source": [
    "## torch 实现 REINFORCE with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "210f9510",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T14:46:26.468080Z",
     "start_time": "2023-08-23T14:46:26.464789Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0c63a2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T14:48:03.325133Z",
     "start_time": "2023-08-23T14:48:03.316118Z"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    '''输入是状态，返回是action的概率分布'''\n",
    "    def __init__(self, num_state, num_action):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(num_state, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_action)\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        prob = F.softmax(x, dim=-1)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50107d84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T14:49:00.128598Z",
     "start_time": "2023-08-23T14:49:00.119585Z"
    }
   },
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "    '''输入是状态，返回是 value（一个scalar）'''\n",
    "    def __init__(self, num_state):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(num_state, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(state))\n",
    "        x = self.fc3(state)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec847c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
