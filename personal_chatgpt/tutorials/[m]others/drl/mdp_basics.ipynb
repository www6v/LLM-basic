{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f77b3dbf",
   "metadata": {},
   "source": [
    "- 这个系列会重新开始介绍强化学习的基础理论\n",
    "    - 基于这个 https://gibberblot.github.io/rl-notes/intro.html\n",
    "- 后边还会再次介绍深度强化学习 DRL\n",
    "    - https://github.com/huggingface/deep-rl-class/tree/main\n",
    "- 最后会跟 llm 结合，我们看 trl：\n",
    "    - https://github.com/huggingface/trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc2e74c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T13:19:28.685219Z",
     "start_time": "2023-12-01T13:19:28.670789Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41d8ec4",
   "metadata": {},
   "source": [
    "## basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fc0255",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal M=(S, A, P, r, \\gamma)\n",
    "$$\n",
    "\n",
    "- 有的地方写作 $(S, s_0, A, P, r, \\gamma)$\n",
    "- $S$：state space，$s_0\\in S$：initial state\n",
    "    - `s0 = mdp.reset()`\n",
    "- $A$: action space, $A(s)\\subseteq A$ applicable in each state (当前状态 $s$ 下允许的动作)\n",
    "- $P_a(s'|s)$ 转移概率（transition probabilities），对于 $s\\in S, a \\in A(s)$\n",
    "    - 有的地方也写作：$P(s'|s,a)$\n",
    "    - 注意是概率化的（也即 probabilistic state model），而非 deterministic（确定性的）的；\n",
    "    - 如果是确定性的，那就是一个经典的序列决策问题了；\n",
    "- $r(s,a,s')$：reward function，可以为正可以为负（设计一个 MDP env，很多一部分都是在设计 reward function）\n",
    "    - $r(s,a)$：确定性的情况下；\n",
    "- $\\gamma$：discount factor ，\n",
    "    - $0\\leq \\gamma < 1$\n",
    "    - 一般又把这一种 MDP 称之为 discounted reward MDP；\n",
    "- MDP 的 solving 是一个序列决策问题（sequence decision making）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6686e2df",
   "metadata": {},
   "source": [
    "### probabilistic state model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf92ee5f",
   "metadata": {},
   "source": [
    "- 抛硬币：heads ($\\frac12$), tails ($\\frac12$)\n",
    "- 掷2个骰子面数和：2 ($\\frac{1}{36}$), 3 ($\\frac1{18}$), 4 ($\\frac{3}{36}$), ..., 12 ($\\frac1{36}$)\n",
    "- 机械臂去拿一个东西：success ($\\frac{4}5$), failure ($\\frac15$)\n",
    "- 打开一个网页：404 概率 1%, 200 概率 99%；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a799e8",
   "metadata": {},
   "source": [
    "### discounted reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cc44b0",
   "metadata": {},
   "source": [
    "- 如果我们的 agent 在与环境的交互过程中(s, a, s', a', ....)，得到这样的一系列reward $r_1, r_2, \\cdots, $，则有 \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "V&=\\gamma^0r_1+\\gamma r_2 + \\gamma^2 r_3+\\gamma^3 r_4+\\cdots\\\\\n",
    "&=r_1+\\gamma (r_2+\\gamma(r_3 + \\gamma(r_4+\\cdots )))\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "V_t=r_t+\\gamma V_{t+1}\n",
    "$$\n",
    "\n",
    "- 递归定义，体现了子问题的结构（$V_t$ 与 $V_{t+1}$ 的关系，动态规划）\n",
    "- $V_{t+1}$ 的价值，通过 $\\gamma V_{t+1}$ 折到现在；\n",
    "- 因为我们要最大化 discounted reward，所以 $\\gamma < 1$ 会隐式地得到一个更短的路径（另外一个角度 action 会有 cost）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff72afd8",
   "metadata": {},
   "source": [
    "## grid world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f3b959",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T13:19:35.158335Z",
     "start_time": "2023-12-01T13:19:35.142604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://gibberblot.github.io/rl-notes/_images/ac08de56caab98830b830b1068f4c5b87881b518b6e0e0ba02e062171bfc21e3.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://gibberblot.github.io/rl-notes/_images/ac08de56caab98830b830b1068f4c5b87881b518b6e0e0ba02e062171bfc21e3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d950232",
   "metadata": {},
   "source": [
    "```\n",
    "class GridWorld(MDP):\n",
    "\n",
    "    ...\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        transitions = []\n",
    "\n",
    "        if state == self.TERMINAL:\n",
    "            if action == self.TERMINATE:\n",
    "                return [(self.TERMINAL, 1.0)]\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        # Probability of not slipping left or right\n",
    "        straight = 1 - (2 * self.noise)\n",
    "\n",
    "        (x, y) = state\n",
    "        if state in self.get_goal_states().keys():\n",
    "            if action == self.TERMINATE:\n",
    "                transitions += [(self.TERMINAL, 1.0)]\n",
    "\n",
    "        elif action == self.UP:\n",
    "            transitions += self.valid_add(state, (x, y + 1), straight)\n",
    "            transitions += self.valid_add(state, (x - 1, y), self.noise)\n",
    "            transitions += self.valid_add(state, (x + 1, y), self.noise)\n",
    "\n",
    "        elif action == self.DOWN:\n",
    "            transitions += self.valid_add(state, (x, y - 1), straight)\n",
    "            transitions += self.valid_add(state, (x - 1, y), self.noise)\n",
    "            transitions += self.valid_add(state, (x + 1, y), self.noise)\n",
    "\n",
    "        elif action == self.RIGHT:\n",
    "            transitions += self.valid_add(state, (x + 1, y), straight)\n",
    "            transitions += self.valid_add(state, (x, y - 1), self.noise)\n",
    "            transitions += self.valid_add(state, (x, y + 1), self.noise)\n",
    "\n",
    "        elif action == self.LEFT:\n",
    "            transitions += self.valid_add(state, (x - 1, y), straight)\n",
    "            transitions += self.valid_add(state, (x, y - 1), self.noise)\n",
    "            transitions += self.valid_add(state, (x, y + 1), self.noise)\n",
    "\n",
    "        # Merge any duplicate outcomes\n",
    "        merged = defaultdict(lambda: 0.0)\n",
    "        for (state, probability) in transitions:\n",
    "            merged[state] = merged[state] + probability\n",
    "\n",
    "        transitions = []\n",
    "        for outcome in merged.keys():\n",
    "            transitions += [(outcome, merged[outcome])]\n",
    "\n",
    "        return transitions\n",
    "\n",
    "    def valid_add(self, state, new_state, probability):\n",
    "        # If the next state is blocked, stay in the same state\n",
    "        if probability == 0.0:\n",
    "            return []\n",
    "\n",
    "        if new_state in self.blocked_states:\n",
    "            return [(state, probability)]\n",
    "\n",
    "        # Move to the next space if it is not off the grid\n",
    "        (x, y) = new_state\n",
    "        if x >= 0 and x < self.width and y >= 0 and y < self.height:\n",
    "            return [((x, y), probability)]\n",
    "\n",
    "        # If off the grid, state in the same state\n",
    "        return [(state, probability)]\n",
    "\n",
    "    def get_reward(self, state, action, new_state):\n",
    "        reward = 0.0\n",
    "        if state in self.get_goal_states().keys() and new_state == self.TERMINAL:\n",
    "            reward = self.get_goal_states().get(state)\n",
    "        else:\n",
    "            reward = self.action_cost\n",
    "        step = len(self.episode_rewards)\n",
    "        self.episode_rewards += [reward * (self.discount_factor ** step)]\n",
    "        return reward\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f91eec",
   "metadata": {},
   "source": [
    "## 与 POMDP、I-POMDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ff534",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "(S, \\Omega, O, A, P, r, \\gamma)\n",
    "$$\n",
    "\n",
    "- POMDP：partial observable MDP\n",
    "    - 就不再是 MDP 默认的全可观测（grid world 的 state 就是自己的位置，(x, y)）\n",
    "    - $s\\in S$: states \n",
    "    - $A(s)\\subseteq A$\n",
    "    - $P_a(s'|s)$\n",
    "    - $r(s,a,s')$\n",
    "    - $\\gamma$\n",
    "    - 观测空间 $\\Omega$\n",
    "    - $O(o|s), o\\in \\Omega$：观测函数\n",
    "    - initial belief state $b_0$\n",
    "- I-POMDP：多智能体环境，I 表示是的 interactive；\n",
    "    - 对状态的建模，就不是只有 physical states，还包含对另一个 agent 的 belief state；"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
