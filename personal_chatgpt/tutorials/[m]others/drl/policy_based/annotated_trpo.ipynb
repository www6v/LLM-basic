{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8485461d-4a06-449f-9362-64b162752a5e",
   "metadata": {},
   "source": [
    "- references\n",
    "    - https://github.com/DeepRLChinese/DeepRL-Chinese/blob/master/09_trpo.py（运行有问题）\n",
    "    - https://medium.com/@vladogim97/trpo-minimal-pytorch-implementation-859e46c4232e\n",
    "        - https://gist.github.com/elumixor/c16b7bdc38e90aa30c2825d53790d217\n",
    "- 对于 DRL 而言\n",
    "    - 神经网络反而是简单的，就是一个超强的 function approximator；训练一个 deep neural network，就是学习一个函数近似器\n",
    "        - $\\pi_\\theta(\\cdot|s)=\\pi_\\theta(a|s)$\n",
    "        - $V(s)$\n",
    "    - 且在 DRL 的问题及应用里，我们需要更灵活多样地组织 learning/training 的 pipeline；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e2af02-3872-4186-b392-4bfaac4411ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.15.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam\n",
    "from IPython.display import Image\n",
    "gym.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42dbd9b-98b1-430b-9865-13d8130314bd",
   "metadata": {},
   "source": [
    "## 基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be39e5e-3254-42f1-89d9-c056513edd63",
   "metadata": {},
   "source": [
    "### gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb9a9ef7-0487-48b1-8a42-37a829238171",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfd108f8-92bb-4fb3-aef6-8aea888d9287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((gym.spaces.box.Box, (4,)), (gym.spaces.discrete.Discrete, Discrete(2)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(type(env.observation_space), env.observation_space.shape), (type(env.action_space), env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d65b0d2-43a0-458d-b611-46a8902060ff",
   "metadata": {},
   "source": [
    "### Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f3c5117-dca0-4910-918a-1ee3e5e4148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 是 type，而非 instance\n",
    "# s, a, r, s'\n",
    "Rollout = namedtuple('Rollout', ['states', 'actions', 'rewards', 'next_states', ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208c422-01ea-4a5c-a2b3-bb823ab6f8b1",
   "metadata": {},
   "source": [
    "### Actor Critic "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1629a3e6-fefb-43b4-8ea3-b15d7ebc4885",
   "metadata": {},
   "source": [
    "- actor: $\\pi_\\theta(a|s)$\n",
    "- critic: value function\n",
    "    - advantage estimation\n",
    "    - 可以是 action-value（Q value），也可以是 state-value，$V(s)$（V value）\n",
    "- advantage estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcf5adc4-478f-4b18-a1d9-4b9b19a36677",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_hidden = 32\n",
    "actor = nn.Sequential(nn.Linear(state_size, actor_hidden),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(actor_hidden, num_actions),\n",
    "                      nn.Softmax(dim=1))\n",
    "\n",
    "# 依概率分布进行采样\n",
    "def get_action(state):\n",
    "    state = torch.tensor(state).float().unsqueeze(0)  # Turn state into a batch with a single element\n",
    "    dist = Categorical(actor(state))  # Create a distribution from probabilities for actions\n",
    "    return dist.sample().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b760d0d1-4ce5-4fb7-b054-4518c3b9f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_hidden = 32\n",
    "critic = nn.Sequential(nn.Linear(state_size, critic_hidden),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(critic_hidden, 1))\n",
    "critic_optimizer = Adam(critic.parameters(), lr=0.005)\n",
    "\n",
    "def update_critic(advantages):\n",
    "    loss = .5 * (advantages ** 2).mean()  # MSE\n",
    "    critic_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e0501ea-b2ba-4f5e-9e19-48ae6ab5b2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../imgs/policy_value_update_summary.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='../imgs/policy_value_update_summary.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f9e872-874a-443f-ba09-a6aecc134c02",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "A_t&=Q_t(s_t,a_t)-V(s_t)\\\\\n",
    "&\\approx R_{t+1}+\\gamma V(s_{t+1}) -V(s_t) \\qquad \\text{TD(0)}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea08cecb-2683-48d4-96ba-cd6b55fc7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_advantages(states, last_state, rewards):\n",
    "    values = critic(states)\n",
    "    last_value = critic(last_state.unsqueeze(0))\n",
    "    next_values = torch.zeros_like(rewards)\n",
    "    for i in reversed(range(rewards.shape[0])):\n",
    "        last_value = next_values[i] = rewards[i] + 0.99 * last_value\n",
    "    advantages = next_values - values\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4421af27-411a-4485-ab9f-68c72ed677e1",
   "metadata": {},
   "source": [
    "## update_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99b24ee7-b36c-4936-8f94-c5345b83e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_agent(rollouts):\n",
    "    states = torch.cat([r.states for r in rollouts], dim=0)\n",
    "    actions = torch.cat([r.actions for r in rollouts], dim=0).flatten()\n",
    "\n",
    "    advantages = [estimate_advantages(states, next_states[-1], rewards) for states, _, rewards, next_states in rollouts]\n",
    "    advantages = torch.cat(advantages, dim=0).flatten()\n",
    "\n",
    "    # Normalize advantages to reduce skewness and improve convergence\n",
    "    advantages = (advantages - advantages.mean()) / advantages.std()  \n",
    "    \n",
    "    update_critic(advantages)\n",
    "\n",
    "    distribution = actor(states)\n",
    "\n",
    "    # Important! We clamp the probabilities, so they do not reach zero\n",
    "    distribution = torch.distributions.utils.clamp_probs(distribution)\n",
    "    \n",
    "    probabilities = distribution[range(distribution.shape[0]), actions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c03207-156d-4454-814e-1872a4a8c27a",
   "metadata": {},
   "source": [
    "## training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7a477aa-c115-40b9-926d-c84ed0e42992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=100, num_rollouts=10):\n",
    "    mean_total_rewards = []\n",
    "    global_rollout = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        rollouts = []\n",
    "        rollout_total_rewards = []\n",
    "        \n",
    "        for t in range(num_rollouts):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "\n",
    "            samples = []\n",
    "\n",
    "            # 一次 trajectory\n",
    "            while not done:\n",
    "                with torch.no_grad():\n",
    "                    action = get_action(state)\n",
    "                    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                    # Collect samples\n",
    "                    samples.append((state, action, reward, next_state))\n",
    "\n",
    "                    state = next_state\n",
    "\n",
    "            # Transpose our samples\n",
    "            states, actions, rewards, next_states = zip(*samples)\n",
    "\n",
    "            states = torch.stack([torch.from_numpy(state) for state in states], dim=0).float()\n",
    "            next_states = torch.stack([torch.from_numpy(state) for state in next_states], dim=0).float()\n",
    "            actions = torch.as_tensor(actions).unsqueeze(1)\n",
    "            rewards = torch.as_tensor(rewards).unsqueeze(1)\n",
    "\n",
    "            rollouts.append(Rollout(states, actions, rewards, next_states))\n",
    "            rollout_total_rewards.append(rewards.sum().item())\n",
    "            global_rollout += 1\n",
    "            \n",
    "        update_agent(rollouts)\n",
    "        mtr = np.mean(rollout_total_rewards)\n",
    "        print(f'E: {epoch}.\\tMean total reward across {num_rollouts} rollouts: {mtr}')\n",
    "        mean_total_rewards.append(mtr)\n",
    "        \n",
    "    plt.plot(mean_total_rewards)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa787ac1-405a-4854-8c62-bd031f7a057a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
