{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87889255-c0de-4af0-ab37-b50c95e45290",
   "metadata": {},
   "source": [
    "- https://zhuanlan.zhihu.com/p/491979794"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe9ac49-ceed-412f-b1a7-418aa16ad08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4960e1f4-bc4f-4557-8454-7bc588a93bc5",
   "metadata": {},
   "source": [
    "## REINFOCE & IS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f58f89-8986-4c8a-9641-bd2214440e89",
   "metadata": {},
   "source": [
    "- IS: Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fea295f-7557-431d-9ca8-cd00af3b9006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../imgs/REINFOCE.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='../imgs/REINFOCE.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900f89d6-b3c9-447a-9768-3dba755d69a1",
   "metadata": {},
   "source": [
    "- 如上图所示，原始的Policy Gradient (PG)需要agent和环境做互动，用得到的奖励信息更新agent，再重新和环境互动。这样每一次互动产生的数据只能被使用一次（On-policy）。为了能够重复利用这些信息，可以使用Importance Sampling。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a179b07e-9cb7-4251-a018-3993f8a63fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://pic1.zhimg.com/80/v2-b35f61037847d0da487594e423a421e4_1440w.webp\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://pic1.zhimg.com/80/v2-b35f61037847d0da487594e423a421e4_1440w.webp', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0703f904-04c1-4886-adeb-58e33e2b9dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://pic1.zhimg.com/v2-1ce4cc236efec6859fe5cad0da05ed8c_r.jpg\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://pic1.zhimg.com/v2-1ce4cc236efec6859fe5cad0da05ed8c_r.jpg', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f237b-ea45-4a88-b4c0-8a05c1bb9bba",
   "metadata": {},
   "source": [
    "- 如果我们从轨迹中的每一个动作状态对 $(s_t,a_t)$ 来考虑的话，而不是定义在整个 trajectory ($\\tau$) 上，重要性采样前的 Policy Gradient 可以写作\n",
    "\n",
    "    $$\n",
    "    \\nabla_\\theta J(\\theta)=\\mathbb E_{(s_t,a_t)\\sim \\pi_\\theta}[\\nabla_\\theta\\log \\pi_\\theta(a_t|s_t)A_\\pi(s_t,a_t)]\n",
    "    $$\n",
    "  - $A_\\pi(s_t,a_t)$ 称为 advantage function；（奖励 $Q$ 减去 baseline）得到；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89f325e4-14dc-457e-8a6b-5572de3a2897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../imgs/pg_is_advantage.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='../imgs/pg_is_advantage.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971fbb12-6738-4728-bd41-12b4b6cf19e6",
   "metadata": {},
   "source": [
    "## PPO-Clipped Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb68af-209e-4098-bbed-6ba555f97648",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "&\\nabla f(x)=f(x)\\nabla \\log f(x)\\\\\n",
    "&\\nabla \\pi_\\theta(a_t|s_t)=\\pi_\\theta(a_t|s_t)\\nabla \\log \\pi_\\theta(a_t|s_t)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1008b46-d94c-4a95-b779-491951b9f37a",
   "metadata": {},
   "source": [
    "- 恢复出原始的目标函数 ($\\nabla f(x)=f(x)\\nabla \\log f(x)$)\n",
    "\n",
    "$$\n",
    "J^{\\theta'}(\\theta)=\\mathbb E_{(s_t,a_t)\\sim \\pi_{\\theta'}}\\left[\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta'}(a_t|s_t)}A_\\pi(s_t,a_t)\\right]\n",
    "$$\n",
    "\n",
    "- 记 $r_t(\\theta)=\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta'}(a_t|s_t)}$，PPO-Clipped Objective的目标函数为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc659500-ec32-4159-9658-24048d0778c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../imgs/ppo_clip.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='../imgs/ppo_clip.png', width=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
