{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d977214f-9889-4c4a-88d3-e3a0bdac3efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8092e7d6-b49c-479a-942b-197255ecc7b9",
   "metadata": {},
   "source": [
    "> 整体再串一遍，强化学习中一大类算法，基于 policy 的算法；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbfaa44-0cdc-40b4-9d07-a571c30f3b84",
   "metadata": {},
   "source": [
    "## trpo vs. policy gradient "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e97bd97-117d-421b-b381-166ef2db8e4b",
   "metadata": {},
   "source": [
    "- cons:\n",
    "    - 计算量更大；\n",
    "- pros:\n",
    "    - 表现更稳定，收敛更快\n",
    "- trust region：\n",
    "    - 数值优化领域很经典的算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc8b852-4625-4218-ada2-cdd6c96d397a",
   "metadata": {},
   "source": [
    "### Gradient Ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42469ebc-6134-4c27-9b24-a31fdbace529",
   "metadata": {},
   "source": [
    "- gradient ascent（最大化问题梯度上升，如果是最小化问题，则用梯度下降）\n",
    "\n",
    "    $$\n",
    "    \\theta^\\star=\\arg\\max_\\theta J(\\theta)\n",
    "    $$\n",
    "\n",
    "    - $g=\\frac{\\partial J(\\theta)}{\\partial \\theta}\\big|_{\\theta=\\theta_{old}}$，在 $\\theta_{old}$ 时，计算梯度；\n",
    "    - $\\theta_{new}=\\theta_{old}+\\alpha\\cdot g$，基于梯度上升，更新参数；\n",
    "    - 直到梯度 $g$ 的二范数接近于0；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b00abe-9a43-44c2-a451-d492669dd799",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Ascent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b3ef9-3877-47f8-996f-112e20c4ae20",
   "metadata": {},
   "source": [
    "- stochastic gradient ascent\n",
    "    - $J(\\theta)=\\mathbb E_S[V(S;\\theta)]$ （$S$ 是随机变量）\n",
    "        - 期望需要做定积分，但定积分可能没有解析解，也就是求不出期望，也就不可能算出梯度；\n",
    "        - 求不出梯度，但可以求出随机梯度（stochastic gradient），随机梯度是对真实梯度的**蒙特卡洛**近似，\n",
    "        - 用随机梯度代替梯度得到的算法就是随机梯度上升（stochastic gradient ascent）；\n",
    "    - SGA\n",
    "        - $s \\Leftarrow$  random sampling\n",
    "        - $g=\\frac{\\partial V(s;\\theta)}{\\partial \\theta}\\big|_{\\theta=\\theta_{old}}$（随机梯度）\n",
    "        - $\\theta_{new}=\\theta_{old}+\\alpha\\cdot g$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05916b89-5fd8-4c8c-bb8f-756396a208a7",
   "metadata": {},
   "source": [
    "### Trust Region （置信域）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d502e0d-c146-4259-8b34-11f7423a63bf",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta^\\star=\\arg\\max_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "- 定义 $\\mathcal N(\\theta_{old})$ 是 $\\theta_{old}$ 的邻域；\n",
    "    - 半径为 $\\delta$ 的圆；\n",
    "  $$\n",
    "  \\mathcal N(\\theta_{old})=\\left\\{\\theta\\big| \\|\\theta-\\theta_{old}\\|\\leq \\delta\\right\\}\n",
    "  $$\n",
    "\n",
    "- 如果相比较复杂的 $J(\\theta)$, $L(\\theta|\\theta_{old})$ 是一个更为简单的函数，且 $L(\\theta|\\theta_{old})$ 能更好地逼近 $J(\\theta)$ 在 $\\mathcal N(\\theta_{old})$，则 $\\mathcal N(\\theta_{old})$ 就可以成为置信域；\n",
    "    - 用 $L(\\theta|\\theta_{old})$ 代替 $J(\\theta)$ 可以让优化变得更容易；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3af79c-d628-425b-a540-4b2d59d53316",
   "metadata": {},
   "source": [
    "- 置信域算法（Trust Regions）重复如下的两步\n",
    "    - Approximation（做近似）：给定 $\\theta_{old}$ 构造 $L(\\theta|\\theta_{old})$ 是在 $\\mathcal N(\\theta_{old})$ 内对 $J(\\theta)$ 的近似\n",
    "        - 比如用 $J(\\theta)$ 的二阶泰勒展开\n",
    "        - 比如用期望的蒙特卡洛近似\n",
    "    - Maximization（最大化）：$\\theta_{new}\\leftarrow \\arg\\max_{\\theta\\in\\mathcal N(\\theta_{old})} L(\\theta|\\theta_{old})$\n",
    "        - 带约束的最大化问题\n",
    "            - 置信域的半径可以发生，我们可以让半径逐渐变小；\n",
    "        - 每一轮都要求解这样一个最大化问题，因此置信域的算法的求解速度不快\n",
    "        - 运算量会比 gd/sgd 要大，其好处在于比梯度算法更稳定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddcc1e08-08a7-448e-a4d1-c333292c4cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/cc/Mmalgorithm.jpg\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://upload.wikimedia.org/wikipedia/commons/c/cc/Mmalgorithm.jpg', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a55cc58-458d-4581-8900-1691252731cf",
   "metadata": {},
   "source": [
    "- $\\theta_m$ 的邻域就是 trust region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "512b650b-e4ac-41ec-b08a-4c4e451d1bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/format:webp/1*c5Z8owKztX2ea52uxdfS5w.jpeg\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://miro.medium.com/v2/format:webp/1*c5Z8owKztX2ea52uxdfS5w.jpeg', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727e7117-b233-44a6-a4bd-ce84d4c91ba8",
   "metadata": {},
   "source": [
    "## policy-based reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1aaee8-c684-42d6-9fd6-78b67cc10b3a",
   "metadata": {},
   "source": [
    "- $\\pi_\\theta(a|s)$ policy network, controlling the agent\n",
    "- $V_\\pi(s)=\\mathbb E_{A\\in \\pi}[Q_\\pi(s,A)]$，状态价值函数（state-value function）\n",
    "    - 对谁求积分，就是消掉谁的过程，比如下面的 $A$\n",
    "$$\n",
    "\\begin{split}\n",
    "V_\\pi(s)&=\\mathbb E_{A\\in\\pi}[Q_\\pi(s,A)]\\\\\n",
    "&=\\sum_a \\pi_\\theta(a|s) Q_\\pi(s,a)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- objective function (maximum)\n",
    "  - 对谁求积分，就是消掉谁的过程，比如下面的 $S$\n",
    "    $$\n",
    "    J(\\theta)=\\mathbb E_S[V_\\pi(S)]\n",
    "    $$\n",
    "    - 策略梯度 PG（Policy Gradient）定理，（过程不再推理）\n",
    "   \n",
    "    $$\n",
    "    \\frac{\\partial J(\\theta)}{\\partial \\theta}=\\mathbb E_S\\left[\\mathbb E_{A\\sim \\pi_\\theta(\\cdot|S)}\\left[\\frac{\\partial \\ln{\\pi_\\theta(A|S)}}{\\partial \\theta} Q_\\pi(S,A)\\right]\\right]\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9139f7e5-3dba-453b-a83b-74b5d83a1c58",
   "metadata": {},
   "source": [
    "- objective function 的推导\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "V_\\pi(S)&=\\sum_a\\pi_\\theta(a|s)Q_\\pi(s,a)\\\\\n",
    "&=\\sum_a\\pi_{\\theta_{old}}(a|s)\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{old}}(a|s)}Q_\\pi(s,a)\\\\\n",
    "&=\\mathbb E_{A\\sim\\pi_{\\theta_{old}}(\\cdot|s)}\\left[\\frac{\\pi_\\theta(A|s)}{\\pi_{\\theta_{old}}(A|s)}Q_\\pi(s,A)\\right]\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- trpo 中最重要的公式：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "J(\\theta)&=\\mathbb E_S[V_\\pi(S)]\\\\\n",
    "&=\\mathbb E_S\\left[\\mathbb E_{A\\sim\\pi_{\\theta_{old}}(\\cdot|s)}\\left[\\frac{\\pi_\\theta(A|s)}{\\pi_{\\theta_{old}}(A|s)}Q_\\pi(s,A)\\right]\\right]\\\\\n",
    "&=\\mathbb E_S\\left[\\mathbb E_{A}\\left[\\frac{\\pi_\\theta(A|s)}{\\pi_{\\theta_{old}}(A|s)}Q_\\pi(s,A)\\right]\\right]\\\\\n",
    "&=\\mathbb E_{S,A}\\left[\\frac{\\pi_\\theta(A|s)}{\\pi_{\\theta_{old}}(A|s)}Q_\\pi(s,A)\\right]\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- $S$ 的随机性来自于状态转移（$S$ is sampled from the **state transition** of the env)\n",
    "    - $p(s_{t+1}|s_t,a_t)$\n",
    "- $A$ 的随机性来自于 $\\pi_{\\theta_{old}}(A|S)$（$A$ is sampled from $\\pi_{\\theta_{old}}(A|S)$）\n",
    "- 对期望 $\\mathbb E_{S,A}$ 做蒙特卡洛近似；\n",
    "    - trajectory（基于 $\\pi_{\\theta_{old}}(\\cdot|s)$）：$s_1,a_1,r_1,...,s_n,a_n,r_n$\n",
    "    - 蒙特卡洛近似（Step 1，approximation）\n",
    " \n",
    "      $$\n",
    "      L(\\theta|\\theta_{old})=\\frac1n\\sum_{i=1}^n\\frac{\\pi_\\theta(a_i|s_i)}{\\pi_{\\theta_{old}}(a_i|s_i)}Q_\\pi(s_i,a_i)\n",
    "      $$\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa7ffdc-1b65-484b-ba72-cfb03af34e48",
   "metadata": {},
   "source": [
    "### TRPO 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127a11d0-2229-4b7f-b801-a113ca2e105a",
   "metadata": {},
   "source": [
    "- 巧妙地将数值优化中的 trust region 应用在策略网络 $\\pi_\\theta(a|s)$ 学习的优化中\n",
    "- policy gradient：\n",
    "    - 对参数敏感；\n",
    "    - 收敛曲线上下震荡；\n",
    "- more robust than policy gradient algorithms\n",
    "- more sample efficient than policy gradient algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92a98c4-5b40-4932-9505-761e110be41b",
   "metadata": {},
   "source": [
    "- Step 1，approximation\n",
    "\n",
    "  $$\n",
    "  L(\\theta|\\theta_{old})=\\frac1n\\sum_{i=1}^n\\frac{\\pi_\\theta(a_i|s_i)}{\\pi_{\\theta_{old}}(a_i|s_i)}Q_\\pi(s_i,a_i)\n",
    "  $$\n",
    "\n",
    "    - 进一步对 $Q_\\pi(s_i,a_i)$ 做蒙特卡洛近似\n",
    "        - 一次 episode 观测到的 rewards：$r_1,r_2,\\cdots,r_n$\n",
    "        - 计算 discounted return（$i$ 时刻起，所有未来时刻的奖励的加权和）\n",
    "\n",
    "          $$\n",
    "          u_i=r_i+\\gamma r_{i+1}+\\gamma^2r_{i+2}+\\cdots+\\gamma^{n-i}r_n=\\sum_{k=i}^n\\gamma^{k-i}r_k\n",
    "          $$\n",
    "        - $Q_\\pi(s_i,a_i)\\approx u_i$\n",
    "\n",
    "  $$\n",
    "  \\begin{split}\n",
    "  J(\\theta)&\\approx L(\\theta|\\theta_{old})=\\frac1n\\sum_{i=1}^n\\frac{\\pi_\\theta(a_i|s_i)}{\\pi_{\\theta_{old}}(a_i|s_i)}Q_\\pi(s_i,a_i)\\\\\n",
    "  &\\approx \\hat L(\\theta|\\theta_{old})=\\frac1n\\sum_{i=1}^n\\frac{\\pi_\\theta(a_i|s_i)}{\\pi_{\\theta_{old}}(a_i|s_i)}u_i\n",
    "  \\end{split}\n",
    "  $$\n",
    "\n",
    "- Step 2, maximization\n",
    "\n",
    "    $$\n",
    "    \\theta_{new}\\leftarrow \\arg\\max_\\theta \\hat L(\\theta|\\theta_{old}), \\quad s.t. \\theta\\in \\mathcal N(\\theta_{old})\n",
    "    $$\n",
    "\n",
    "    - 如何衡量 $\\theta$ 与 $\\theta_{old}$ 的距离\n",
    "        - $\\|\\theta-\\theta_{old}\\|\\leq \\Delta$\n",
    "        - $\\frac1n\\sum_{i=1}^nKL[\\pi_{\\theta_{old}}(\\cdot|s_i)\\|\\pi_{\\theta}(\\cdot|s_i)]\\leq \\Delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e82018-462e-4bed-8f28-e4dba74b1690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../imgs/trpo_summary.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='../imgs/trpo_summary.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae15e7b-a82c-44ed-aab8-d10df9c6f974",
   "metadata": {},
   "source": [
    "## cg solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2828b07-361c-44dc-8aee-246a67cb7616",
   "metadata": {},
   "source": [
    "- Hvp: hessian-vector product\n",
    "    - 一个特殊的矩阵矢量乘法，特殊在这个矩阵是 Hessian 矩阵；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2cf76889-5453-41e5-bab3-b0cbb8bc3136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:700/format:webp/0*7deI_HpCbAquF58S.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the search direction (netwon's method)\n",
    "Image(url='https://miro.medium.com/v2/resize:fit:700/format:webp/0*7deI_HpCbAquF58S.png', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cade20a-1dd9-4bb0-b655-d8ece08f85b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../../../imgs/cg.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='../../../imgs/cg.png', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a17315af-a899-4891-b890-15170744836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc70e2c-554e-4545-8281-50f907943382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [1., 3.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[4, 1], [1, 3]], dtype=torch.float32)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab0fae60-b049-4103-8606-86dd81a27566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([1, 2], dtype=torch.float32)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "17d6e5d3-0427-46a2-a538-431a62632bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Avp_fun(v):\n",
    "    return torch.matmul(A, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eb410284-5d4f-44d8-bc90-a3f64d0115bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cg_solver(Avp_fun, x0, b, max_iter=10):\n",
    "    \"\"\"\n",
    "    Finds an approximate solution to a set of linear equations Ax = b\n",
    "    Parameters\n",
    "    ----------\n",
    "    Avp_fun : callable\n",
    "        a function that right multiplies a matrix A by a vector\n",
    "    b : torch.FloatTensor\n",
    "        the right hand term in the set of linear equations Ax = b\n",
    "    max_iter : int\n",
    "        the maximum number of iterations (default is 10)\n",
    "    Returns\n",
    "    -------\n",
    "    x : torch.FloatTensor\n",
    "        the approximate solution to the system of equations defined by Avp_fun\n",
    "        and b\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    r = b - (A @ x)\n",
    "    p = r.clone()\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        Avp = Avp_fun(p)\n",
    "\n",
    "        alpha = torch.matmul(r, r) / torch.matmul(p, Avp)\n",
    "        \n",
    "        # delta < eps\n",
    "        if torch.isnan(alpha):\n",
    "            return x\n",
    "            \n",
    "        x += alpha * p\n",
    "        print('iter', i, alpha, x)\n",
    "        if i == max_iter - 1:\n",
    "            return x\n",
    "\n",
    "        r_new = r - alpha * Avp\n",
    "        beta = torch.matmul(r_new, r_new) / torch.matmul(r, r)\n",
    "        r = r_new\n",
    "        p = r + beta * p\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "678fdf06-1e33-4dc0-ba7e-088e73af0855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-8., -3.])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1., 2.]) - A @ torch.tensor([2., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d5385ea6-7ce8-4aab-8940-3e4da401406e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f5e5eb68-e2ea-4b4c-8ef3-1b7d171bbd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 tensor(0.2205) tensor([0.2356, 0.3384])\n",
      "iter 1 tensor(0.4122) tensor([0.0909, 0.6364])\n",
      "iter 2 tensor(0.2500) tensor([0.0909, 0.6364])\n",
      "iter 3 tensor(0.3636) tensor([0.0909, 0.6364])\n",
      "iter 4 tensor(0.2203) tensor([0.0909, 0.6364])\n",
      "iter 5 tensor(0.4127) tensor([0.0909, 0.6364])\n",
      "iter 6 tensor(0.2222) tensor([0.0909, 0.6364])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0909, 0.6364])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = torch.tensor([2., 1.])\n",
    "x = cg_solver(Avp_fun, x0, b)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3f6dbfb7-e07f-4e71-862a-63fc67872990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0909, 0.6364])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.inv(A) @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b2a8dd8-fe80-4eb2-a48d-34c1176b9248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(A @ x, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
