{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0f9f91-96d1-45a2-951e-90811f3dd6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdad9a3f-db44-4f07-b8fc-2545df874bd0",
   "metadata": {},
   "source": [
    "- references\n",
    "    - https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/129053162"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd5c2a2-c2e4-4c38-afb3-e6cd9c43f6db",
   "metadata": {},
   "source": [
    "## loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea0d3f0-cee7-4dd9-a3a0-86b78d0ae3e3",
   "metadata": {},
   "source": [
    "- pg_loss + value_loss\n",
    "    - pg_loss 是 PPO 中 actor 的 loss 函数，其通过 discount reward 和 importance ratio 来计算当前 step 的 reward 应该是多少：\n",
    "    - value_loss 是 PPO 中 critic 的 loss 函数，其目的在于评判每一个 token 被生成后的 value 是多少。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b94065-0737-435d-aec0-e1adab2df35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../imgs/pg_loss.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='../imgs/pg_loss.png', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7487fea0-4c3e-4d92-8326-614b33321e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../imgs/value_loss.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='../imgs/value_loss.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab829e9d-4714-4e36-8bd8-0a2d6269b6d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## batched_forward_pass(queries, responses, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc150abf-2ed3-4fe3-83bb-7c71dceb5a6f",
   "metadata": {},
   "source": [
    "### input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27f1d61-16c7-4142-89a0-8c50340c981d",
   "metadata": {},
   "source": [
    "- `queries`: list\n",
    "    - len(queries) == 1024 (batch size)\n",
    "    - `[q.shape for q in queries]`\n",
    "- `responses`: list\n",
    "    - len(responses) == len(queries)\n",
    "    - `[r.shape for r in responses]`\n",
    "- `scores`: tensor\n",
    "    - `scores.shape == torch.Size([1024])`\n",
    "- `model_inputs = self.prepare_model_inputs(queries, responses)`: \n",
    "    - `model_inputs.keys() = dict_keys(['input_ids', 'attention_mask'])`\n",
    "    - `model_inputs['attention_mask'].sum(dim=-1)`\n",
    "        - len(r) + len(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b5252-0613-4495-b682-c9a5954cae8b",
   "metadata": {},
   "source": [
    "```\n",
    "model_inputs = self.prepare_model_inputs(queries, responses)\n",
    "all_logprobs, _, values, masks = self.batched_forward_pass(self.model, queries, responses, model_inputs, ...)\n",
    "ref_logprobs, _, _, _ = self.batched_forward_pass(self.ref_model, queries, responses, model_inputs, ...)\n",
    "```\n",
    "\n",
    "- `all_logprobs.shape == torch.Size([1024, 21])`\n",
    "    - logp，已做过 gather\n",
    "- `ref_logprobs.shape == torch.Size([1024, 21])`\n",
    "    - logp，已做过 gather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e98b167-e35f-40f6-a4c4-0099138972af",
   "metadata": {},
   "source": [
    "## compute_rewards(scores, all_logprobs, ref_logprobs, masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f50485-d560-47c2-9939-25b3845205a6",
   "metadata": {},
   "source": [
    "- kl penalty\n",
    "\n",
    "    $$\n",
    "    D_{KL}(P||Q)=\\sum_{x}P(x)\\log\\frac{P(x)}{Q(x)}\n",
    "    $$\n",
    "\n",
    "    - `logprob - ref_logprob`（相对）\n",
    "\n",
    "    $$\n",
    "    \\log p-\\log q=\\log \\frac{p}{q}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fd4afb-e942-4fe6-9be2-3e51c86d4c9c",
   "metadata": {},
   "source": [
    "### kl_ctl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27170c4-49ba-41b0-89c6-4b700ca8f3ca",
   "metadata": {},
   "source": [
    "```\n",
    "# self.kl_ctl = AdaptiveKLController(0.02, 6, 10000)\n",
    "self.kl_ctl = AdaptiveKLController(self.config.init_kl_coef, self.config.target, self.config.horizon)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30fd514-ad2b-478d-a78e-fee469bf6cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl.trainer import AdaptiveKLController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df0f7cf0-2341-47aa-96cf-571410ea343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1909.08593.pdf, 2.2\n",
    "kl_ctl = AdaptiveKLController(0.02, 6, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f396bbe5-522b-4b9a-b1a8-b08489920a75",
   "metadata": {},
   "source": [
    "### $R(x,y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58ecb4c-5c18-4bc4-9f62-e0e6ab98d77a",
   "metadata": {},
   "source": [
    "$$\n",
    "R(x,y)=r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\rho(y|x)}\n",
    "$$\n",
    "\n",
    "```\n",
    "reward = score - self.kl_ctl.value * kl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50fbf66-21e7-404d-a728-1435adc08bdb",
   "metadata": {},
   "source": [
    "## compute_advantages(**values**, rewards, masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb79600-6bd7-4232-a999-5ce5627202b5",
   "metadata": {},
   "source": [
    "- `values` from value head (active model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c9b86b-e2be-46c7-a3e4-ea8daf34ea35",
   "metadata": {},
   "source": [
    "- lam is the GAE’s $λ$ parameter.\n",
    "    - Generalized Advantage Estimation (GAE)\n",
    "- returns = advantages + values\n",
    "    - advantages = $r+\\gamma V_{next} - V_{current}$\n",
    "    - `values` from value head (active model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb69644-2a05-4c9f-b6ef-4933b83083ef",
   "metadata": {},
   "source": [
    "## train_minibatch && loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1105504c-f3b3-44fc-913e-7e9cbc8e947d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
