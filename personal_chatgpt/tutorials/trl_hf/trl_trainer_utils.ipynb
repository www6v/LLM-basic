{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ba4c210",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f0b932",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T06:54:28.359455Z",
     "start_time": "2024-03-17T06:54:21.325753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-17 14:54:24,055] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from trl.trainer import ConstantLengthDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a2f6dc",
   "metadata": {},
   "source": [
    "- ConstantLengthDataset\n",
    "    - seq_length=args.seq_length,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbe2b84",
   "metadata": {},
   "source": [
    "## `prepare_model_for_kbit_training`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dc9fc8",
   "metadata": {},
   "source": [
    "This method wraps the entire protocol for preparing a model before running a training. \n",
    "This includes:\n",
    "- 1- Cast the layernorm in fp32 \n",
    "- 2- making output embedding layer require grads \n",
    "- 3- Add the upcasting of the lm head to fp32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb81402",
   "metadata": {},
   "source": [
    "```\n",
    "if not is_gptq_quantized:\n",
    "    # cast all non INT8 parameters to fp32\n",
    "    for param in model.parameters():\n",
    "        if (param.dtype == torch.float16) or (param.dtype == torch.bfloat16):\n",
    "            param.data = param.data.to(torch.float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec3d1d",
   "metadata": {},
   "source": [
    "## peft_module_casting_to_bf16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a23f6a",
   "metadata": {},
   "source": [
    "```\n",
    "def peft_module_casting_to_bf16(model):\n",
    "    from peft.tuners.tuners_utils import BaseTunerLayer\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, BaseTunerLayer):\n",
    "            module = module.to(torch.bfloat16)\n",
    "        elif isinstance(module, torch.nn.LayerNorm) or \"norm\" in name:\n",
    "            module = module.to(torch.float32)\n",
    "        elif any(x in name for x in [\"lm_head\", \"embed_tokens\", \"wte\", \"wpe\"]):\n",
    "            if hasattr(module, \"weight\"):\n",
    "                if module.weight.dtype == torch.float32:\n",
    "                    module = module.to(torch.bfloat16)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
