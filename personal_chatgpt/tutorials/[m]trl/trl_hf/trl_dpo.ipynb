{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa570992-7300-4e41-a673-f54a828b562e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T03:32:21.115694Z",
     "iopub.status.busy": "2024-06-23T03:32:21.115113Z",
     "iopub.status.idle": "2024-06-23T03:32:21.126054Z",
     "shell.execute_reply": "2024-06-23T03:32:21.124280Z",
     "shell.execute_reply.started": "2024-06-23T03:32:21.115648Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c1857f-7020-4157-b22e-c1df7755e56f",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27fd7a5-f687-4274-b921-7af8ab56b72a",
   "metadata": {},
   "source": [
    "- misc\n",
    "    - https://github.com/huggingface/alignment-handbook/tree/main\n",
    "- 3 steps\n",
    "    - pre-training a large language model (LLM) to predict the next token on internet-scale data, on clusters of thousands of GPUs. One calls the result a **\"base model\"**\n",
    "    - supervised fine-tuning (SFT) to turn the base model into a useful assistant (ChatBot)\n",
    "        - we turned a \"base model\" into a useful assistant, by training it to **generate useful completions given human instructions.**\n",
    "    - human preference fine-tuning which increases the assistant's friendliness, helpfulness and safety.\n",
    "        - \"safe\", \"friendly\", \"harmless\", \"inclusive\",\n",
    "        - human preference fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74ea9ae-cabd-46d4-8041-10d0d5c7f768",
   "metadata": {},
   "source": [
    "### align & why align"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139c761-3081-4fa0-8c22-06d7f4867863",
   "metadata": {},
   "source": [
    "- dpo: direct preference optimization your language model is **secretly a reward model**\n",
    "    - https://arxiv.org/abs/2305.18290\n",
    "- collect human/ai feedback to learn $p(y_w\\gt y_l)$\n",
    "- RLHF - the OG（Original Gangster，始祖） of LLM alignment\n",
    "\n",
    "    $$\n",
    "    \\max_{\\pi_\\theta} \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_\\theta(y \\mid x)} \\underbrace{\\left[ r_\\phi(x, y) \\right]}_{\\text{maximise rewards}} - \\underbrace{\\beta \\mathbb{D}_{\\text{KL}} \\left[ \\pi_\\theta(y \\mid x) \\parallel \\pi_{\\text{ref}}(y \\mid x) \\right]}_{\\text{use KL penalty to prevent\n",
    "    reward hacking (controlled by β)\n",
    "    }}\n",
    "    $$\n",
    "    - RL（PPO）很多超参，且训练不稳定；\n",
    "    - 还需要一个RM（Reward model），这样的话一共三个model要操作，actor model，ref model，Reward model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0f1568-00e2-44d4-a5d5-0f14ccf35fd5",
   "metadata": {},
   "source": [
    "### reward hacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547053db-5844-42d9-bbee-f7f7eb493058",
   "metadata": {},
   "source": [
    "- The alignment Problem（《人机对齐》）\n",
    "    - University of Toronto economist Joshua Gans wanted to enlist the help of his older daughter in potty training her younger brother. So he did what any good economist would do. He offered her an incentive: anytime she helped her brother go to the bathroom, she would get a piece of candy. The daughter immediately found a loophole that her father, the economics professor, had overlooked. “I realized that the more that goes in, the more comes out,” she says. “So I was just feeding my brother buckets and buckets of water.” Gans affirms: “It didn’t really work out too well.”\n",
    "        - 多伦多大学经济学家乔舒亚·甘斯（Joshua Gans）的一次亲身经历。他想让大女儿帮忙训练她的小弟弟使用厕所，于是他做了一个经济学家常做的事情：提供激励。他告诉女儿，每次她帮助弟弟上厕所，她都会得到一块糖果。女儿立刻发现了一个父亲——这位经济学教授——没有注意到的漏洞。她说：“我意识到，喝的越多，排的也越多。”于是她开始给弟弟大量灌水。甘斯证实道：“这并没有取得很好的效果。”\n",
    "    - 指的是在给定奖励机制下，个体通过**非预期的方式**最大化奖励的行为。\n",
    "- This constraint is added to avoid what is known as “reward hacking”: the language model (the policy) may just choose sequences of tokens that achieve high reward but may be total gibberish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4729fd48-8bbb-44ee-b89b-5d253edf7901",
   "metadata": {},
   "source": [
    "### Bradley-Terry model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a80aa4-8c6c-4649-9e3b-547ab30e3bb2",
   "metadata": {},
   "source": [
    "- convert the preferences into a score (reward)\n",
    "\n",
    "    $$\n",
    "    P(y_w > y_l) = \\frac{e^{r^*(x, y_w)}}{e^{r^*(x, y_w)} + e^{r^*(x, y_l)}}\n",
    "    $$\n",
    "\n",
    "- 显然希望最大化这个概率，即 $y_w\\gt y_\\ell$ 的概率尽可能地高，也就是基于 MLE 的方式求解 RM 的参数；\n",
    "\n",
    "\n",
    "    $$\n",
    "    P(y_w > y_l) = \\frac{e^{r_\\phi(x, y_w)}}{e^{r_\\phi(x, y_w)} + e^{r_\\phi(x, y_l)}}=\\sigma(r_\\phi(x, y_w)-r_\\phi(x, y_l))\n",
    "    $$\n",
    "\n",
    "  - 因为有：$\\frac{e^A}{e^A+e^B}=\\frac{1}{1+e^{B-A}}=\\frac{1}{1+e^{-(A-B)}}=\\sigma(A-B)$\n",
    "\n",
    "- reward model loss\n",
    "\n",
    "    \n",
    "    $$\n",
    "    L=-\\mathbb E_{(x,y_w,y_l)\\sim D}\\left[\\log\\sigma\\left(r_\\phi(x,y_w)-r_\\phi(x,w_l)\\right)\\right]\n",
    "    $$\n",
    "\n",
    "- policy objective\n",
    "  \n",
    "    $$\n",
    "    \\max_{\\pi_\\theta} \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_\\theta(y \\mid x)} \\underbrace{\\left[ r_\\phi(x, y) \\right]}_{\\text{maximise rewards}} - \\underbrace{\\beta \\mathbb{D}_{\\text{KL}} \\left[ \\pi_\\theta(y \\mid x) \\parallel \\pi_{\\text{ref}}(y \\mid x) \\right]}_{\\text{use KL penalty to prevent\n",
    "    reward hacking (controlled by β)\n",
    "    }}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ff774a-8517-442b-b674-f93efd88c474",
   "metadata": {},
   "source": [
    "### RLHF objective => DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7e9e12-45a5-44fe-8019-7915c17cafa9",
   "metadata": {},
   "source": [
    "$$\n",
    "J_{RLHF} = \\max_{\\pi_\\theta} \\mathbb{E}_{x \\sim D, y \\sim \\pi_\\theta(y|x)} \\left[ r_\\phi(x, y) - \\beta \\mathbb{D}_{KL}\\left[ \\pi_\\theta(y|x) \\parallel \\pi_{ref}(y|x) \\right] \\right]\n",
    "$$\n",
    "\n",
    "- 不可以直接通过 gradient descent 的方式来做优化，因为 $y\\sim \\pi_\\theta(y|x)$（采样的过程，也包含了很多的策略，比如 greedy，beam-search ...）\n",
    "\n",
    "DPO paper eq3 -> eq4，求得解析解（$Z(x) = \\sum_y \\pi_{\\text{ref}}(y|x) \\exp \\left( \\frac{1}{\\beta} r(x, y) \\right)$）；\n",
    "\n",
    "$$\n",
    "\\pi_r(y|x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp \\left( \\frac{1}{\\beta} r(x, y) \\right)\n",
    "$$\n",
    "\n",
    "进一步我们推导 $r(x,y)$\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\log \\pi^*(y|x)&= \\log \\left[ \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp \\left( \\frac{1}{\\beta} r(x, y) \\right) \\right] \\\\\n",
    "&= \\log \\pi_{\\text{ref}}(y|x) - \\log Z(x) + \\log \\exp \\left( \\frac{1}{\\beta} r(x, y) \\right) \\\\\n",
    "&= \\log \\pi_{\\text{ref}}(y|x) - \\log Z(x) + \\frac{1}{\\beta} r(x, y)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "因此：\n",
    "\n",
    "$$\n",
    "r(x, y) = \\beta \\log \\frac{\\pi^*(y|x)}{\\pi_{\\text{ref}}(y|x)} + \\beta \\log Z(x)\n",
    "$$\n",
    "\n",
    "再来回顾下 Bradley-Terry model\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "p(y_w\\gt y_l)&=\\sigma(r(x,y_w)-r(x,y_l))\\\\\n",
    "&=\\sigma\\left(\\beta \\log \\frac{\\pi^*(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} + \\beta \\log Z(x) - \\beta \\log \\frac{\\pi^*(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}- \\beta \\log Z(x)\\right)\\\\\n",
    "&=\\sigma\\left(\\beta \\log \\frac{\\pi^*(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}-\\beta \\log \\frac{\\pi^*(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "最终 DPO：\n",
    "\n",
    "$$\n",
    "L_{DPO}(\\pi_{\\theta}; \\pi_{\\text{ref}}) = -\\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_{\\theta}(y_w | x)}{\\pi_{\\text{ref}}(y_w | x)} - \\beta \\log \\frac{\\pi_{\\theta}(y_l | x)}{\\pi_{\\text{ref}}(y_l | x)} \\right) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41379511-1866-410a-af9a-e609320f5085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a086b96-47b3-4f68-b6b0-daae6f0f4665",
   "metadata": {},
   "source": [
    "## DPO（Direct Preference Optimization）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b346b491-0df4-47c4-82f9-5d6b57d6811b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T03:32:35.294975Z",
     "iopub.status.busy": "2024-06-23T03:32:35.294447Z",
     "iopub.status.idle": "2024-06-23T03:32:35.312076Z",
     "shell.execute_reply": "2024-06-23T03:32:35.310258Z",
     "shell.execute_reply.started": "2024-06-23T03:32:35.294931Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/rlhf_dpo.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/rlhf_dpo.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff27b752-39a3-4887-b7eb-6616e1257fbc",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "&\\max_{\\pi} \\mathbb{E}_{(x, y_w, y_l) \\sim D} \\log \\sigma \\left( \\beta \\log \\frac{\\pi(y_w | x)}{\\pi_{\\text{ref}}(y_w | x)} - \\beta \\log \\frac{\\pi(y_l | x)}{\\pi_{\\text{ref}}(y_l | x)} \\right)\\\\\n",
    "&\\log \\sigma \\left(\\beta\\left(\\log\\frac{\\pi(y_w|x)}{\\pi(y_l|x)}-\\log\\frac{\\pi_{\\text{ref}}(y_w|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\right)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- trains to assign high probability to positive examples $\\pi_\\theta(y_w|x)$ and low probability to negative examples $\\pi_\\theta(y_l|x)$ \n",
    "- only two models (actor/active model, reference model (sft))\n",
    "    - $\\beta$ is a temperature parameter for the DPO loss, typically something in the range of 0.1 to 0.5.\n",
    "- 求导练习\n",
    "\n",
    "    $$\n",
    "    \\left(\\log\\sigma(z)\\right))'=\\frac{1}{\\sigma(z)}\\cdot \\sigma(z)(1-\\sigma(z))=1-\\sigma(z)=\\sigma(-z)\n",
    "    $$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} \\mathcal{L}_{\\text{DPO}} (\\pi_{\\theta}; \\pi_{\\text{ref}}) = & -\\beta \\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\underbrace{\\sigma \\left( \\hat{r}_{\\theta}(x, y_l) - \\hat{r}_{\\theta}(x, y_w) \\right)}_{\\text{higher weight when reward estimate is wrong} } \\left[ \\underbrace{\\nabla_{\\theta} \\log \\pi(y_w | x)}_{\\text{increase likelihood of } y_w} - \\underbrace{\\nabla_{\\theta} \\log \\pi(y_l | x)}_{\\text{decrease likelihood of } y_l} \\right] \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- $\\hat r_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}$（implicit reward from LM）\n",
    "    - 它表示的是模型 $\\pi_\\theta$ 相对于参考模型 $\\pi_{\\text{ref}}$ 对生成结果 $y$ 的偏好程度。\n",
    "    - 与显式奖励（例如通过人工评分或者明确的奖励函数给出的奖励）不同，隐式奖励是通过模型内部的概率分布计算得到的。在DPO中，这种隐式奖励直接来源于模型本身的输出概率分布，因此称为“隐式奖励”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf81006e-6215-480f-9a76-235ed38ae224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F487af2f0-e51d-4140-92a7-23476c5ea016_1600x1015.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F487af2f0-e51d-4140-92a7-23476c5ea016_1600x1015.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1869cb8-02bf-4671-8f7e-53dcd31af57b",
   "metadata": {},
   "source": [
    "## practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42d068f-0230-4664-bae3-13a51ea852fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2cbb16-2856-47d0-a274-705588b09825",
   "metadata": {},
   "source": [
    "- https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama_2/scripts/\n",
    "    - dpo_llama2.py\n",
    "\n",
    "```\n",
    "accelerate launch examples/research_projects/stack_llama_2/scripts/dpo_llama2.py \\\n",
    "    --model_name_or_path=\"sft/final_checkpoint\" \\\n",
    "    --output_dir=\"dpo\"\n",
    "```\n",
    "\n",
    "- basemodel: `meta-llama/Llama-2-7b-hf`\n",
    "    - 非 chat/instruct 版\n",
    "- sft:\n",
    "- dpo (alignment => rlhf):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd346a9-26e0-4e6f-a96a-8f785a238b0d",
   "metadata": {},
   "source": [
    "- dataset\n",
    "    - lvwerra/stack-exchange-paired\n",
    "        - question, Create pairs (response_j, response_k) where j was rated better than k\n",
    "        - train: \"data/rl\"\n",
    "            - 1652614 条样本\n",
    "            - tokenize 的时候要指定 `num_proc`，以充分利用 cpu 核心/线程级别的分布式，加速数据的预处理\n",
    "        - evaluation: \"data/evaluation\"\n",
    "            - 242 条？？（开启 sanity_check），先保证程序运行没有bug；\n",
    "    - process\n",
    "        - num_proc: 开启 cpu 的多进程，会显著地提升大数据集的预处理效率；\n",
    "          \n",
    "        ```\n",
    "        {\n",
    "            'prompt': List[str],\n",
    "            'chosen': List[str],\n",
    "            'rejected': List[str],\n",
    "        }\n",
    "        return dataset.map(\n",
    "            return_prompt_and_responses,\n",
    "            batched=True,\n",
    "            num_proc=num_proc,\n",
    "            remove_columns=original_columns,\n",
    "        )\n",
    "        ```\n",
    "        \n",
    "- 关于参数\n",
    "    - `total_train_batch_size` = `self._train_batch_size * args.gradient_accumulation_steps * args.world_size`\n",
    "    - `max_steps`: Total optimization steps\n",
    "- `DPOTrainer`\n",
    "\n",
    "    ```\n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model,\n",
    "        ref_model=None,\n",
    "    ```\n",
    "    \n",
    "    - 不需要 `ref_model`\n",
    "    - `model = get_peft_model(model, peft_config)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15276a1d-e600-44fa-a386-67b341849ff7",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc30bbd-9116-4003-b223-0a59065b7406",
   "metadata": {},
   "source": [
    "\n",
    "concatenated_forward\n",
    "\n",
    "- concatenated_input_ids: shape, `[4, 259]`\n",
    "- all_logits = model('concatenated_input_ids', ).logits\n",
    "    - `[4, 259, 32000]`\n",
    "- all_logps = get_batch_logps(all_logits, concatenated_labels)\n",
    "    - torch.gather\n",
    "    - `[4]`\n",
    "  \n",
    "```\n",
    "chosen_logps = all_logps[:len_chosen]\n",
    "rejected_logps = all_logps[len_chosen:]\n",
    "\n",
    "chosen_logits = all_logits[:len_chosen]\n",
    "rejected_logits = all_logits[len_chosen:]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e356a6-3bab-4525-9306-b0ff6c9cc53e",
   "metadata": {},
   "source": [
    "dpo loss\n",
    "\n",
    "```\n",
    "losses, chosen_rewards, rejected_rewards = self.dpo_loss(\n",
    "    policy_chosen_logps,\n",
    "    policy_rejected_logps,\n",
    "    reference_chosen_logps,\n",
    "    reference_rejected_logps,\n",
    ")\n",
    "```\n",
    "\n",
    "- $\\pi_{\\log \\text{ratios}}=\\pi_{\\text{chosen}}-\\pi_{\\text{rejected}}$\n",
    "- $\\rho_{\\log \\text{ratios}}=\\rho_{\\text{chosen}}-\\rho_{\\text{rejected}}$\n",
    "- $\\text{logits} = \\pi_{\\log \\text{ratios}} - \\rho_{\\log \\text{ratios}}$\n",
    "- loss\n",
    "    - sigmoid\n",
    "      \n",
    "      $$\n",
    "      \\text{losses} = -\\log \\sigma(\\beta \\cdot \\text{logits}) \\cdot (1 - \\alpha) - \\log \\sigma(-\\beta \\cdot \\text{logits}) \\cdot \\alpha\n",
    "      $$\n",
    "      - $\\alpha$：label_smoothing parameter\n",
    "    - hinge:\n",
    "\n",
    "      $$\n",
    "      \\text{losses} = \\max(0, 1 - \\beta \\cdot \\text{logits})\n",
    "      $$\n",
    "      \n",
    "    - ipo:\n",
    "\n",
    "      $$\n",
    "      \\text{losses} = \\left( \\text{logits} - \\frac{1}{2\\beta} \\right)^2\n",
    "      $$\n",
    "      \n",
    "    - kto pair (https://arxiv.org/abs/2402.01306)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
