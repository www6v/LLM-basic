{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d02e776-6006-4228-bbf4-3a6bc9967311",
   "metadata": {},
   "source": [
    "> PPO: default RL algo in OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e64fff97-e1e0-443d-ba0a-3cde7665d679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae87b4e-3e1e-4818-9c1b-ae7af926ad72",
   "metadata": {},
   "source": [
    "## math tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83140a8-70ef-4be0-9dac-e1eae5455b7b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla f(x)=f(x)\\nabla \\log f(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69541fb9-2a7a-437e-b605-afbe4703110e",
   "metadata": {},
   "source": [
    "## DQN -> TRPO -> PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b279c3-0892-47d4-abab-2c68bfc0d78f",
   "metadata": {},
   "source": [
    "- DQN (2014)\n",
    "    - unstable & offline method\n",
    "- TRPO（2015）: Trust Region Policy Optimization\n",
    "\n",
    "- PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a9dda2-d810-4f3f-894c-846073beca8c",
   "metadata": {},
   "source": [
    "## policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbab84a-d98a-4c27-b8a2-8e6abc8f4f79",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\nabla_\\theta J(\\pi_\\theta)=E_{\\tau\\sim \\pi}\\left[\\sum_{t=0}^T\\nabla_\\theta\\log \\pi_\\theta(a_t|s_t)G_t\\right]\\\\\n",
    "G_t=R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2} + \\cdots = \\sum_{k=t}^T\\gamma^{k-t}R_k\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- $G_t$:  reward-to-go (RTG).\n",
    "    - $R_k$：表示时刻 $k$ 的即时回报；\n",
    "    - 从当前时刻 （$t$）起，未来某个时间（$T$）点之前的所有回报（reward）的累计和。\n",
    "    - 常用于策略梯度（policy gradient）中；\n",
    "    - 计算每个行动的优势函数时，RTG减去基线可以更有效地估算行动的价值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55804489-1a6c-4946-98f0-26566606602a",
   "metadata": {},
   "source": [
    "```\n",
    "def compute_rtgs(self, batch_rews):\n",
    "    # The rewards-to-go (rtg) per episode per batch to return.\n",
    "    # The shape will be (num timesteps per episode)\n",
    "    batch_rtgs = []\n",
    "    # Iterate through each episode backwards to maintain same order\n",
    "    # in batch_rtgs\n",
    "    for ep_rews in reversed(batch_rews):\n",
    "        discounted_reward = 0 # The discounted reward so far\n",
    "        for rew in reversed(ep_rews):\n",
    "            discounted_reward = rew + discounted_reward * self.gamma\n",
    "            batch_rtgs.insert(0, discounted_reward)\n",
    "    # Convert the rewards-to-go into a tensor\n",
    "    batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n",
    "    return batch_rtgs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa98c5-e805-472a-b35b-cdcbfc5a0616",
   "metadata": {},
   "source": [
    "- Advantage function.\n",
    "\n",
    "$$\n",
    "A^\\pi(s,a)=Q^\\pi(s,a)-V_{\\phi_k}(s)\n",
    "$$\n",
    "\n",
    "\n",
    "```\n",
    "def evaluate(self, batch_obs):\n",
    "    # Query critic network for a value V for each obs in batch_obs.\n",
    "    V = self.critic(batch_obs).squeeze()\n",
    "    return V\n",
    "  \n",
    "# Calculate V_{phi, k}\n",
    "V = self.evaluate(batch_obs)\n",
    "# ALG STEP 5\n",
    "# Calculate advantage\n",
    "A_k = batch_rtgs - V.detach()\n",
    "\n",
    "# Normalize advantages\n",
    "A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf27f81-7578-41b2-adbd-82124536d68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbdf8007-92e9-4db3-bf1a-a57a6db98a82",
   "metadata": {
    "tags": []
   },
   "source": [
    "## on policy vs. off policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd24e56-ed58-4000-a2d7-fc3842c914ba",
   "metadata": {},
   "source": [
    "- Policy gradient的方法，一般是on policy的，ppo通过importance sampling的方式，将其变为off policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecdbda55-3003-4d08-90aa-97bd6e58a917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*ONEmrwcr-jOwyUlFoO1g-Q.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://miro.medium.com/v2/resize:fit:4800/format:webp/1*ONEmrwcr-jOwyUlFoO1g-Q.png', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1078d830-bbea-4d5f-8b49-4ac0451dd0fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://huggingface.co/blog/assets/73_deep_rl_q_part2/off-on-4.jpg\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://huggingface.co/blog/assets/73_deep_rl_q_part2/off-on-4.jpg', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fe26e6-90d2-42c0-8de1-081333a49ad1",
   "metadata": {},
   "source": [
    "## PPO clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9528ed-c11e-40fe-b06f-532f5eda8f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
