{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d18b6bb1",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "421703e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T12:44:35.735694Z",
     "start_time": "2023-07-11T12:44:34.828171Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed8aad4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T12:44:40.609485Z",
     "start_time": "2023-07-11T12:44:35.738839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0795c743a24269b5848f92c6279e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('./codeparrot/', split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7547aa8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T12:44:40.614116Z",
     "start_time": "2023-07-11T12:44:40.611102Z"
    }
   },
   "outputs": [],
   "source": [
    "iter_dataset = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24793c15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T12:44:51.739732Z",
     "start_time": "2023-07-11T12:44:40.615982Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b23ac8c",
   "metadata": {},
   "source": [
    "### build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c13fc6f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T12:44:51.752898Z",
     "start_time": "2023-07-11T12:44:51.745819Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "bytes_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_bytes_map = dict((v, k) for k, v in bytes_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_bytes_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e94ed7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T12:44:51.775985Z",
     "start_time": "2023-07-11T12:44:51.756206Z"
    }
   },
   "outputs": [],
   "source": [
    "length = 100000\n",
    "def batch_iterator(batch_size=1000):\n",
    "#     for _ in tqdm(range(0, len(dataset), batch_size)):\n",
    "    for _ in tqdm(range(0, length, batch_size)):\n",
    "        yield [next(iter_dataset)['content'] for _ in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d75cc046",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T12:58:58.766198Z",
     "start_time": "2023-07-11T12:44:51.780093Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:24<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 比较耗时\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), \n",
    "                                                  vocab_size=12500, \n",
    "                                                  initial_alphabet=base_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e6b5524",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T12:58:58.778472Z",
     "start_time": "2023-07-11T12:58:58.769645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=12500, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0753a31",
   "metadata": {},
   "source": [
    "### new tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cc90ac2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T13:02:10.452326Z",
     "start_time": "2023-07-11T13:02:10.396047Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = sorted(new_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37f8ef65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T13:02:39.926867Z",
     "start_time": "2023-07-11T13:02:39.916585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65f2900c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T13:07:08.931776Z",
     "start_time": "2023-07-11T13:07:08.917947Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ĠĠ', '  '),\n",
       " ('ĠĠĠĠ', '    '),\n",
       " ('ĠĠĠ', '   '),\n",
       " ('ĠĠĠĠĠĠĠĠ', '        '),\n",
       " ('se', 'se'),\n",
       " ('in', 'in'),\n",
       " ('ĠĠĠĠĠĠĠ', '       '),\n",
       " ('re', 're'),\n",
       " ('on', 'on'),\n",
       " ('te', 'te'),\n",
       " ('ĊĠĠĠĠĠĠĠ', '\\n       '),\n",
       " ('ĊĠĠĠĠĠĠĠĠ', '\\n        '),\n",
       " ('or', 'or'),\n",
       " ('st', 'st'),\n",
       " ('de', 'de'),\n",
       " ('ĊĠĠĠ', '\\n   '),\n",
       " ('th', 'th'),\n",
       " ('le', 'le'),\n",
       " ('Ġ=', ' ='),\n",
       " ('lf', 'lf'),\n",
       " ('self', 'self'),\n",
       " ('me', 'me'),\n",
       " ('al', 'al')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 257:280\n",
    "[(t, new_tokenizer.convert_tokens_to_string([t])) for t, _ in tokens[257:280]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fc17664",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T13:07:43.437279Z",
     "start_time": "2023-07-11T13:07:43.424262Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ĠSetup', ' Setup'),\n",
       " ('publisher', 'publisher'),\n",
       " ('DER', 'DER'),\n",
       " ('Ġcapt', ' capt'),\n",
       " ('Ġembedded', ' embedded'),\n",
       " ('Ġregarding', ' regarding'),\n",
       " ('Bundle', 'Bundle'),\n",
       " ('355', '355'),\n",
       " ('Ġrecv', ' recv'),\n",
       " ('Ġdmp', ' dmp'),\n",
       " ('Ġvault', ' vault'),\n",
       " ('ĠMongo', ' Mongo'),\n",
       " ('Ġpossibly', ' possibly'),\n",
       " ('implementation', 'implementation'),\n",
       " ('Matches', 'Matches')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last 15\n",
    "[(t, new_tokenizer.convert_tokens_to_string([t])) for t, _ in tokens[-15:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d5966b",
   "metadata": {},
   "source": [
    "### python keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54d1cc6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T13:08:30.360002Z",
     "start_time": "2023-07-11T13:08:30.353807Z"
    }
   },
   "outputs": [],
   "source": [
    "# python 标准库\n",
    "import keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2f56afa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T13:08:37.320294Z",
     "start_time": "2023-07-11T13:08:37.310118Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keyword.kwlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a45d501b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T13:09:31.704315Z",
     "start_time": "2023-07-11T13:09:31.509426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`await` not in the new tokenizer\n",
      "`finally` not in the new tokenizer\n",
      "`nonlocal` not in the new tokenizer\n"
     ]
    }
   ],
   "source": [
    "for kw in keyword.kwlist:\n",
    "    if kw not in new_tokenizer.vocab:\n",
    "        print(f'`{kw}` not in the new tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae26470",
   "metadata": {},
   "source": [
    "### retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50432c82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T13:12:31.682216Z",
     "start_time": "2023-07-11T13:10:45.911502Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:48<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "length = 100000*2\n",
    "def batch_iterator(batch_size=1000):\n",
    "    for _ in tqdm(range(0, length, batch_size)):\n",
    "        yield [next(iter_dataset)['content'] for _ in range(batch_size)]\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), \n",
    "                                                  vocab_size=32768, \n",
    "                                                  initial_alphabet=base_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1684c7ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T13:14:08.664271Z",
     "start_time": "2023-07-11T13:14:08.237918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`nonlocal` not in the new tokenizer\n"
     ]
    }
   ],
   "source": [
    "for kw in keyword.kwlist:\n",
    "    if kw not in new_tokenizer.vocab:\n",
    "        print(f'`{kw}` not in the new tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea169d",
   "metadata": {},
   "source": [
    "### to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49afbb7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T13:19:41.416879Z",
     "start_time": "2023-07-11T13:19:40.802405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/lanchunhui/codeparrot', endpoint='https://huggingface.co', repo_type='model', repo_id='lanchunhui/codeparrot')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from huggingface_hub import create_repo\n",
    "# create_repo('codeparrot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0bfee7a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T13:28:54.217590Z",
     "start_time": "2023-07-11T13:28:54.211301Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf441c01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T13:30:34.662059Z",
     "start_time": "2023-07-11T13:28:55.913474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/lanchunhui/codeparrot/commit/212e703fb744884f9563cfbf6de94ffd5792606a', commit_message='Upload tokenizer', commit_description='', oid='212e703fb744884f9563cfbf6de94ffd5792606a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = 'codeparrot'\n",
    "org = 'lanchunhui'\n",
    "new_tokenizer.push_to_hub(ckpt, organization=org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2013cea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
