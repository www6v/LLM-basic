{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b19b81b9",
   "metadata": {},
   "source": [
    "## basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e5c2aa",
   "metadata": {},
   "source": [
    "- https://arxiv.org/pdf/1604.06174.pdf\n",
    "- https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9\n",
    "    - **computation graph examples**\n",
    "    - 节点在必要时才加载进来\n",
    "    - At the peak, the algorithm stores all activations,\n",
    "        - forward\n",
    "        - $O(n)$ memory requirement for network of depth $n$.\n",
    "    - recomputing them later.\n",
    "        - More generally, this “memory-poor” strategy needs $O(1)$ memory but requires $O(n^2)$ computation steps.\n",
    "    - https://yaroslavvb.medium.com/backprop-and-systolic-arrays-24e925d2050\n",
    "- https://huggingface.co/docs/transformers/v4.18.0/en/performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f3dfc",
   "metadata": {},
   "source": [
    "- 依然是显存占用优化算法\n",
    "    - 当然是 memory usage 与 computation time 之间的 tradeoff ；\n",
    "- In deep neural networks, backpropagation requires storing **intermediate activations** for computing gradients during the backward pass. \n",
    "    - 但是当层数变多时，存储所有的中间层的激活值（intermediate activations）非常地占用显存；\n",
    "- gradient checkpointing 选择性地重新计算（recompute）一部分的 intermediate activations 在反向传播过程中来缓解显存的压力；\n",
    "    - Instead of storing all activations (**during the forward pass**), only a **subset** of them, typically those necessary for computing gradients, are cached. \n",
    "    - The remaining intermediate activations are recomputed on-the-fly **during the backward pass**. By recomputing rather than storing all intermediate activations, memory usage is reduced at the cost of increased computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728436a",
   "metadata": {},
   "source": [
    "## Trainer Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5338d7d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:34:15.237019Z",
     "start_time": "2023-08-01T14:34:15.231063Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58676ab6",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e28440",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:10:40.914071Z",
     "start_time": "2023-08-01T14:10:38.784576Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "seq_len, dataset_size = 512, 512\n",
    "dummy_data = {\n",
    "    \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "    \"labels\": np.random.randint(0, 2, (dataset_size)),\n",
    "}\n",
    "ds = Dataset.from_dict(dummy_data)\n",
    "ds.set_format(\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8873293",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:10:42.310407Z",
     "start_time": "2023-08-01T14:10:42.306034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512) (512,)\n"
     ]
    }
   ],
   "source": [
    "print(dummy_data['input_ids'].shape, dummy_data['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f1a8c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:10:43.139461Z",
     "start_time": "2023-08-01T14:10:43.126892Z"
    }
   },
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd25e1f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:10:45.226598Z",
     "start_time": "2023-08-01T14:10:45.200548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 355 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decca142",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f7b9f52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:10:56.447776Z",
     "start_time": "2023-08-01T14:10:50.041113Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 2511 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-large-uncased').to('cuda')\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75f899",
   "metadata": {},
   "source": [
    "### training without checkpint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10a898fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:11:01.015211Z",
     "start_time": "2023-08-01T14:11:00.899833Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, logging\n",
    "\n",
    "default_args = {\n",
    "    \"output_dir\": \"tmp\",\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"none\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9667cbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:10:23.706296Z",
     "start_time": "2023-08-01T14:09:59.763317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128/128 00:22, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 23.78\n",
      "Samples/second: 21.53\n",
      "GPU memory occupied: 12917 MB.\n"
     ]
    }
   ],
   "source": [
    "# logging.set_verbosity_error()\n",
    "# 跑 training with checkpint 时，需要把这段代码注释掉\n",
    "training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16221954",
   "metadata": {},
   "source": [
    "### training with checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c541409",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:11:42.048122Z",
     "start_time": "2023-08-01T14:11:02.940907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128/128 00:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 38.96\n",
      "Samples/second: 13.14\n",
      "GPU memory occupied: 9305 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args\n",
    ")\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b701047b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:13:56.688736Z",
     "start_time": "2023-08-01T14:13:56.681213Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# steps(total batches)\n",
    "512/((1*1) * 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7603f98",
   "metadata": {},
   "source": [
    "## pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a08296",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T14:43:46.649782Z",
     "start_time": "2023-08-09T14:43:45.377053Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint_sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf5c80e",
   "metadata": {},
   "source": [
    "## huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5671adb1",
   "metadata": {},
   "source": [
    "- checkpoint default module\n",
    "\n",
    "```\n",
    "class BertPreTrainedModel(PreTrainedModel):\n",
    "    def _set_gradient_checkpointing(self, module, value=False):\n",
    "        # 只对 BertEncoder 进行 checkpoint\n",
    "        if isinstance(module, BertEncoder):\n",
    "            module.gradient_checkpointing = value\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff62430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
