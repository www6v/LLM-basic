{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d03e420-b850-4779-9da2-7003157f329f",
   "metadata": {},
   "source": [
    "\n",
    "## <center> DeepSeek R1 Distill高效微调入门实战"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3c2264-ad51-4d26-ad78-17f1a61148c7",
   "metadata": {},
   "source": [
    "### 一、unsloth快速使用入门"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a790496-d887-4d14-bf36-aeb2a4b0a944",
   "metadata": {},
   "source": [
    "#### 1.借助unsloth进行模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3bcd26a2-92f4-477c-8db5-f330add5d33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/jupyterfile/wei\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.internal-mirrors.ucloud.cn/simple\n",
      "Requirement already satisfied: unsloth in /home/ubuntu/miniconda3/lib/python3.12/site-packages (2025.2.15)\n",
      "Requirement already satisfied: unsloth_zoo>=2025.2.7 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (2025.2.7)\n",
      "Requirement already satisfied: torch>=2.4.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (2.6.0)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (0.0.29.post3)\n",
      "Requirement already satisfied: bitsandbytes in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (0.45.2)\n",
      "Requirement already satisfied: triton>=3.0.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (3.2.0)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (23.2)\n",
      "Requirement already satisfied: tyro in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (0.9.16)\n",
      "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (4.49.0)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (3.3.2)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (4.66.4)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (6.0.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (0.43.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (2.2.3)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (1.4.0)\n",
      "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (0.15.1)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (0.14.0)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (0.29.1)\n",
      "Requirement already satisfied: hf_transfer in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (0.32.2)\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth) (0.21.0)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from accelerate>=0.34.1->unsloth) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from accelerate>=0.34.1->unsloth) (0.5.2)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth) (2.32.2)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth) (3.11.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from huggingface_hub->unsloth) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.0)\n",
      "Requirement already satisfied: rich in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (13.9.4)\n",
      "Requirement already satisfied: cut_cross_entropy in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth_zoo>=2025.2.7->unsloth) (25.1.1)\n",
      "Requirement already satisfied: pillow in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from unsloth_zoo>=2025.2.7->unsloth) (11.1.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from diffusers->unsloth) (8.6.1)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from tyro->unsloth) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from tyro->unsloth) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from tyro->unsloth) (4.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (24.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2024.6.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (2.18.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=2.4.0->unsloth) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "! pip install unsloth\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847f228-7910-43d6-91c6-515b9d9e2ef3",
   "metadata": {},
   "source": [
    "- 尝试用unsloth进行LLama模型推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37319bd-28b9-4c00-9c24-bd7ed75cd922",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先设置关键参数，并读取模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4608195-1e9d-47c6-8c50-e752da195fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "# load_in_4bit = False\n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c826a-4fe6-4ea2-a67d-9b78b7a9425e",
   "metadata": {},
   "source": [
    "> 注，若显存不足，则可以load_in_4bit = True，运行4 bit量化版。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "825bc87f-731e-4ccc-8b75-2815a0ea1038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.151 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/deepseek-ai/DeepSeek-R1-Distill-Llama-8B does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"./model/deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea8ca04-2d64-40a3-9df0-89e9dbaca0fe",
   "metadata": {},
   "source": [
    "> 在INT4量化情况下，8B模型推理仅需7G左右显存。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e19d8-365d-4464-badb-1bc1d6ec77dd",
   "metadata": {},
   "source": [
    "此时model就是读取进来的DeepSeek R1 8B蒸馏模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f687092f-94c7-450a-b251-fceb26e5a716",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd41d797-5505-415d-93f8-2e6adaf4f961",
   "metadata": {},
   "source": [
    "而tokenizer则是分词器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "46bac590-0f5b-4c71-98ea-320c00d69b12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='./model/deepseek-ai/DeepSeek-R1-Distill-Llama-8B', vocab_size=128000, model_max_length=131072, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<｜end▁of▁sentence｜>', 'pad_token': '<|finetune_right_pad_id|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t128000: AddedToken(\"<｜begin▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128001: AddedToken(\"<｜end▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128002: AddedToken(\"<|reserved_special_token_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128003: AddedToken(\"<|reserved_special_token_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128004: AddedToken(\"<|finetune_right_pad_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128005: AddedToken(\"<|reserved_special_token_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128006: AddedToken(\"<|start_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128007: AddedToken(\"<|end_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128008: AddedToken(\"<|eom_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128009: AddedToken(\"<|eot_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128010: AddedToken(\"<|python_tag|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128011: AddedToken(\"<｜User｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t128012: AddedToken(\"<｜Assistant｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t128013: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t128014: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t128015: AddedToken(\"<｜▁pad▁｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128016: AddedToken(\"<|reserved_special_token_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128017: AddedToken(\"<|reserved_special_token_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128018: AddedToken(\"<|reserved_special_token_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128019: AddedToken(\"<|reserved_special_token_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128020: AddedToken(\"<|reserved_special_token_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128021: AddedToken(\"<|reserved_special_token_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128022: AddedToken(\"<|reserved_special_token_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128023: AddedToken(\"<|reserved_special_token_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128024: AddedToken(\"<|reserved_special_token_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128025: AddedToken(\"<|reserved_special_token_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128026: AddedToken(\"<|reserved_special_token_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128027: AddedToken(\"<|reserved_special_token_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128028: AddedToken(\"<|reserved_special_token_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128029: AddedToken(\"<|reserved_special_token_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128030: AddedToken(\"<|reserved_special_token_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128031: AddedToken(\"<|reserved_special_token_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128032: AddedToken(\"<|reserved_special_token_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128033: AddedToken(\"<|reserved_special_token_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128034: AddedToken(\"<|reserved_special_token_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128035: AddedToken(\"<|reserved_special_token_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128036: AddedToken(\"<|reserved_special_token_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128037: AddedToken(\"<|reserved_special_token_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128038: AddedToken(\"<|reserved_special_token_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128039: AddedToken(\"<|reserved_special_token_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128040: AddedToken(\"<|reserved_special_token_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128041: AddedToken(\"<|reserved_special_token_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128042: AddedToken(\"<|reserved_special_token_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128043: AddedToken(\"<|reserved_special_token_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128044: AddedToken(\"<|reserved_special_token_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128045: AddedToken(\"<|reserved_special_token_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128046: AddedToken(\"<|reserved_special_token_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128047: AddedToken(\"<|reserved_special_token_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128048: AddedToken(\"<|reserved_special_token_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128049: AddedToken(\"<|reserved_special_token_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128050: AddedToken(\"<|reserved_special_token_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128051: AddedToken(\"<|reserved_special_token_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128052: AddedToken(\"<|reserved_special_token_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128053: AddedToken(\"<|reserved_special_token_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128054: AddedToken(\"<|reserved_special_token_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128055: AddedToken(\"<|reserved_special_token_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128056: AddedToken(\"<|reserved_special_token_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128057: AddedToken(\"<|reserved_special_token_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128058: AddedToken(\"<|reserved_special_token_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128059: AddedToken(\"<|reserved_special_token_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128060: AddedToken(\"<|reserved_special_token_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128061: AddedToken(\"<|reserved_special_token_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128062: AddedToken(\"<|reserved_special_token_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128063: AddedToken(\"<|reserved_special_token_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128064: AddedToken(\"<|reserved_special_token_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128065: AddedToken(\"<|reserved_special_token_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128066: AddedToken(\"<|reserved_special_token_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128067: AddedToken(\"<|reserved_special_token_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128068: AddedToken(\"<|reserved_special_token_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128069: AddedToken(\"<|reserved_special_token_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128070: AddedToken(\"<|reserved_special_token_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128071: AddedToken(\"<|reserved_special_token_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128072: AddedToken(\"<|reserved_special_token_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128073: AddedToken(\"<|reserved_special_token_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128074: AddedToken(\"<|reserved_special_token_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128075: AddedToken(\"<|reserved_special_token_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128076: AddedToken(\"<|reserved_special_token_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128077: AddedToken(\"<|reserved_special_token_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128078: AddedToken(\"<|reserved_special_token_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128079: AddedToken(\"<|reserved_special_token_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128080: AddedToken(\"<|reserved_special_token_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128081: AddedToken(\"<|reserved_special_token_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128082: AddedToken(\"<|reserved_special_token_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128083: AddedToken(\"<|reserved_special_token_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128084: AddedToken(\"<|reserved_special_token_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128085: AddedToken(\"<|reserved_special_token_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128086: AddedToken(\"<|reserved_special_token_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128087: AddedToken(\"<|reserved_special_token_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128088: AddedToken(\"<|reserved_special_token_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128089: AddedToken(\"<|reserved_special_token_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128090: AddedToken(\"<|reserved_special_token_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128091: AddedToken(\"<|reserved_special_token_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128092: AddedToken(\"<|reserved_special_token_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128093: AddedToken(\"<|reserved_special_token_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128094: AddedToken(\"<|reserved_special_token_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128095: AddedToken(\"<|reserved_special_token_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128096: AddedToken(\"<|reserved_special_token_88|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128097: AddedToken(\"<|reserved_special_token_89|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128098: AddedToken(\"<|reserved_special_token_90|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128099: AddedToken(\"<|reserved_special_token_91|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128100: AddedToken(\"<|reserved_special_token_92|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128101: AddedToken(\"<|reserved_special_token_93|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128102: AddedToken(\"<|reserved_special_token_94|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128103: AddedToken(\"<|reserved_special_token_95|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128104: AddedToken(\"<|reserved_special_token_96|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128105: AddedToken(\"<|reserved_special_token_97|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128106: AddedToken(\"<|reserved_special_token_98|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128107: AddedToken(\"<|reserved_special_token_99|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128108: AddedToken(\"<|reserved_special_token_100|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128109: AddedToken(\"<|reserved_special_token_101|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128110: AddedToken(\"<|reserved_special_token_102|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128111: AddedToken(\"<|reserved_special_token_103|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128112: AddedToken(\"<|reserved_special_token_104|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128113: AddedToken(\"<|reserved_special_token_105|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128114: AddedToken(\"<|reserved_special_token_106|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128115: AddedToken(\"<|reserved_special_token_107|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128116: AddedToken(\"<|reserved_special_token_108|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128117: AddedToken(\"<|reserved_special_token_109|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128118: AddedToken(\"<|reserved_special_token_110|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128119: AddedToken(\"<|reserved_special_token_111|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128120: AddedToken(\"<|reserved_special_token_112|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128121: AddedToken(\"<|reserved_special_token_113|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128122: AddedToken(\"<|reserved_special_token_114|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128123: AddedToken(\"<|reserved_special_token_115|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128124: AddedToken(\"<|reserved_special_token_116|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128125: AddedToken(\"<|reserved_special_token_117|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128126: AddedToken(\"<|reserved_special_token_118|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128127: AddedToken(\"<|reserved_special_token_119|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128128: AddedToken(\"<|reserved_special_token_120|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128129: AddedToken(\"<|reserved_special_token_121|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128130: AddedToken(\"<|reserved_special_token_122|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128131: AddedToken(\"<|reserved_special_token_123|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128132: AddedToken(\"<|reserved_special_token_124|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128133: AddedToken(\"<|reserved_special_token_125|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128134: AddedToken(\"<|reserved_special_token_126|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128135: AddedToken(\"<|reserved_special_token_127|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128136: AddedToken(\"<|reserved_special_token_128|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128137: AddedToken(\"<|reserved_special_token_129|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128138: AddedToken(\"<|reserved_special_token_130|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128139: AddedToken(\"<|reserved_special_token_131|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128140: AddedToken(\"<|reserved_special_token_132|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128141: AddedToken(\"<|reserved_special_token_133|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128142: AddedToken(\"<|reserved_special_token_134|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128143: AddedToken(\"<|reserved_special_token_135|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128144: AddedToken(\"<|reserved_special_token_136|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128145: AddedToken(\"<|reserved_special_token_137|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128146: AddedToken(\"<|reserved_special_token_138|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128147: AddedToken(\"<|reserved_special_token_139|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128148: AddedToken(\"<|reserved_special_token_140|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128149: AddedToken(\"<|reserved_special_token_141|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128150: AddedToken(\"<|reserved_special_token_142|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128151: AddedToken(\"<|reserved_special_token_143|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128152: AddedToken(\"<|reserved_special_token_144|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128153: AddedToken(\"<|reserved_special_token_145|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128154: AddedToken(\"<|reserved_special_token_146|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128155: AddedToken(\"<|reserved_special_token_147|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128156: AddedToken(\"<|reserved_special_token_148|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128157: AddedToken(\"<|reserved_special_token_149|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128158: AddedToken(\"<|reserved_special_token_150|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128159: AddedToken(\"<|reserved_special_token_151|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128160: AddedToken(\"<|reserved_special_token_152|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128161: AddedToken(\"<|reserved_special_token_153|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128162: AddedToken(\"<|reserved_special_token_154|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128163: AddedToken(\"<|reserved_special_token_155|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128164: AddedToken(\"<|reserved_special_token_156|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128165: AddedToken(\"<|reserved_special_token_157|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128166: AddedToken(\"<|reserved_special_token_158|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128167: AddedToken(\"<|reserved_special_token_159|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128168: AddedToken(\"<|reserved_special_token_160|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128169: AddedToken(\"<|reserved_special_token_161|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128170: AddedToken(\"<|reserved_special_token_162|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128171: AddedToken(\"<|reserved_special_token_163|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128172: AddedToken(\"<|reserved_special_token_164|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128173: AddedToken(\"<|reserved_special_token_165|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128174: AddedToken(\"<|reserved_special_token_166|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128175: AddedToken(\"<|reserved_special_token_167|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128176: AddedToken(\"<|reserved_special_token_168|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128177: AddedToken(\"<|reserved_special_token_169|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128178: AddedToken(\"<|reserved_special_token_170|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128179: AddedToken(\"<|reserved_special_token_171|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128180: AddedToken(\"<|reserved_special_token_172|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128181: AddedToken(\"<|reserved_special_token_173|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128182: AddedToken(\"<|reserved_special_token_174|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128183: AddedToken(\"<|reserved_special_token_175|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128184: AddedToken(\"<|reserved_special_token_176|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128185: AddedToken(\"<|reserved_special_token_177|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128186: AddedToken(\"<|reserved_special_token_178|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128187: AddedToken(\"<|reserved_special_token_179|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128188: AddedToken(\"<|reserved_special_token_180|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128189: AddedToken(\"<|reserved_special_token_181|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128190: AddedToken(\"<|reserved_special_token_182|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128191: AddedToken(\"<|reserved_special_token_183|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128192: AddedToken(\"<|reserved_special_token_184|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128193: AddedToken(\"<|reserved_special_token_185|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128194: AddedToken(\"<|reserved_special_token_186|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128195: AddedToken(\"<|reserved_special_token_187|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128196: AddedToken(\"<|reserved_special_token_188|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128197: AddedToken(\"<|reserved_special_token_189|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128198: AddedToken(\"<|reserved_special_token_190|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128199: AddedToken(\"<|reserved_special_token_191|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128200: AddedToken(\"<|reserved_special_token_192|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128201: AddedToken(\"<|reserved_special_token_193|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128202: AddedToken(\"<|reserved_special_token_194|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128203: AddedToken(\"<|reserved_special_token_195|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128204: AddedToken(\"<|reserved_special_token_196|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128205: AddedToken(\"<|reserved_special_token_197|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128206: AddedToken(\"<|reserved_special_token_198|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128207: AddedToken(\"<|reserved_special_token_199|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128208: AddedToken(\"<|reserved_special_token_200|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128209: AddedToken(\"<|reserved_special_token_201|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128210: AddedToken(\"<|reserved_special_token_202|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128211: AddedToken(\"<|reserved_special_token_203|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128212: AddedToken(\"<|reserved_special_token_204|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128213: AddedToken(\"<|reserved_special_token_205|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128214: AddedToken(\"<|reserved_special_token_206|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128215: AddedToken(\"<|reserved_special_token_207|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128216: AddedToken(\"<|reserved_special_token_208|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128217: AddedToken(\"<|reserved_special_token_209|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128218: AddedToken(\"<|reserved_special_token_210|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128219: AddedToken(\"<|reserved_special_token_211|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128220: AddedToken(\"<|reserved_special_token_212|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128221: AddedToken(\"<|reserved_special_token_213|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128222: AddedToken(\"<|reserved_special_token_214|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128223: AddedToken(\"<|reserved_special_token_215|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128224: AddedToken(\"<|reserved_special_token_216|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128225: AddedToken(\"<|reserved_special_token_217|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128226: AddedToken(\"<|reserved_special_token_218|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128227: AddedToken(\"<|reserved_special_token_219|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128228: AddedToken(\"<|reserved_special_token_220|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128229: AddedToken(\"<|reserved_special_token_221|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128230: AddedToken(\"<|reserved_special_token_222|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128231: AddedToken(\"<|reserved_special_token_223|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128232: AddedToken(\"<|reserved_special_token_224|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128233: AddedToken(\"<|reserved_special_token_225|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128234: AddedToken(\"<|reserved_special_token_226|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128235: AddedToken(\"<|reserved_special_token_227|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128236: AddedToken(\"<|reserved_special_token_228|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128237: AddedToken(\"<|reserved_special_token_229|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128238: AddedToken(\"<|reserved_special_token_230|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128239: AddedToken(\"<|reserved_special_token_231|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128240: AddedToken(\"<|reserved_special_token_232|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128241: AddedToken(\"<|reserved_special_token_233|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128242: AddedToken(\"<|reserved_special_token_234|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128243: AddedToken(\"<|reserved_special_token_235|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128244: AddedToken(\"<|reserved_special_token_236|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128245: AddedToken(\"<|reserved_special_token_237|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128246: AddedToken(\"<|reserved_special_token_238|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128247: AddedToken(\"<|reserved_special_token_239|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128248: AddedToken(\"<|reserved_special_token_240|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128249: AddedToken(\"<|reserved_special_token_241|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128250: AddedToken(\"<|reserved_special_token_242|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128251: AddedToken(\"<|reserved_special_token_243|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128252: AddedToken(\"<|reserved_special_token_244|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128253: AddedToken(\"<|reserved_special_token_245|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128254: AddedToken(\"<|reserved_special_token_246|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128255: AddedToken(\"<|reserved_special_token_247|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7998c2-3505-45e6-98bd-e913609b0530",
   "metadata": {},
   "source": [
    "将模型调整为推理模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "443cad53-991e-4904-8c03-e06259020939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a80018-7759-44b1-9ec1-88c5a62a1e7c",
   "metadata": {},
   "source": [
    "然后即可和模型进行对话："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "126c1a9f-12a7-4144-afc3-3f0d7ab47fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"请问如何证明根号2是无理数？\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd9cdc7-9f6e-466a-8a8d-b88ce06b69ec",
   "metadata": {},
   "source": [
    "然后这里我们首先需要借助分词器，将输入的问题转化为标记索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f85aa14b-f4f2-45c8-90bd-238b875b16a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([question], return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ae01d5b0-542b-4d58-9209-bf5f265a2039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,  15225,  57107, 109425, 125544, 102831,  18476,     17,  21043,\n",
       "          43292,  22649,   9039,  11571]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c2f92-5814-47a2-9c8f-244d828344b3",
   "metadata": {},
   "source": [
    "最后再带入inputs进行对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0e2d68a5-56c3-478d-9c6b-63095fe38660",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ce6c0-871f-42dc-a6d6-ca694464580d",
   "metadata": {},
   "source": [
    "此时得到的回复也是词索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5aedeeac-5163-4a3a-b33e-038ed93bf1fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,  15225,  57107,  ...,     13,    220,  78388]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f33d9-ebbd-4ed9-bce5-14802d18ec67",
   "metadata": {},
   "source": [
    "同样需要分词器将其转化为文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "328a281f-cc79-4e79-b765-b12603f23040",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "080d7add-cfa5-4a5a-88bc-e1b498fd050e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<｜begin▁of▁sentence｜>请问如何证明根号2是无理数？或者有没有其他方法？\\n\\n嗯，我现在要证明根号2是无理数，对吧？或者有没有其他方法？首先，我记得证明√2是无理数的方法是通过假设它是有理数，然后推导出矛盾。让我先回忆一下这个过程。\\n\\n好，首先假设√2是一个有理数，也就是可以表示成两个整数的比值，也就是√2 = a/b，其中a和b都是正整数，并且b不等于0，而且这个分数已经是最简形式，也就是说a和b没有除了1以外的公约数。\\n\\n接下来，我可以把这个等式两边平方，得到2 = a² / b²，也就是a² = 2b²。这样的话，a²必须是2的倍数，因为右边是2b²，而b²是整数，所以a²也是整数，进而a也是整数。\\n\\n既然a²是2的倍数，那么a也必须是2的倍数，对吗？因为如果一个数的平方是偶数，那么这个数本身也必须是偶数。所以，a可以表示为2k，其中k是一个整数。\\n\\n现在，我把a代入原来的等式√2 = a/b，也就是√2 = (2k)/b。那么，两边平方得到2 = (4k²)/b²，进而得到b² = 2k²。\\n\\n同样地，这意味着b²也是2的倍数，所以b也必须是2的倍数。也就是说，b可以表示为2m，其中m是一个整数。\\n\\n现在，我已经有了a = 2k和b = 2m，代入√2 = a/b，也就是√2 = (2k)/(2m) = k/m。这样的话，√2 = k/m，其中k和m都是整数，而且k和m也是互质的，因为原来的a和b已经是最简形式。\\n\\n现在，这就导致了一个问题，因为如果√2是一个有理数，也就是可以表示成两个整数的比值，那么它必然是最简形式的，也就是k和m互质。但是，根据上面的推导，√2 = k/m，而k和m都必须是偶数吗？\\n\\n不对，等一下，k和m是否是偶数呢？因为a = 2k，所以k可以是任何整数，同样，b = 2m，所以m也是整数。现在，假设k和m都是偶数，那么我们可以继续分解下去，得到k = 2n，m = 2p，这样的话，√2 = (2n)/(2p) = n/p，这样就会导致√2 = n/p，其中n和p都是整数，这样就可以无限继续下去，这意味着√2可以无限分解下去，这显然是不对的，因为任何数都不能无限分解下去，除非它为0，而√2显然不等于0。\\n\\n因此，这说明我们的假设有问题，也就是说，如果√2是一个有理数，那么它就可以被表示成一个无限可分的比值，这显然是不可能的，所以我们的初始假设就是错误的，也就是说√2不是有理数，因此√2是无理数。\\n\\n或者，我可以换一种方式来思考。假设√2是有理数，那么它可以写成两个整数的比值a/b，其中a和b没有公因数。然后平方两边得到2 = a² / b²，所以a² = 2b²。这样的话，a必须是b的√2倍，也就是说，a必须是√2倍的b，而√2是无理数，这就意味着a也是无理数，这与我们的假设a是有理数矛盾，所以原来的假设是错误的，因此√2是无理数。\\n\\n另外，有没有其他方法来证明√2是无理数呢？我记得还有一种方法是通过代数的方式，假设√2是有理数，然后引入矛盾。或者，也可以使用二次方程的解的性质来证明。\\n\\n比如，考虑方程x² - 2 = 0，它的解是x = ±√2。如果这个方程有有理数解，那么根据有理根定理，解必须是±p/q，其中p和q是整数，而分母q必须是1，也就是说，p必须是±√2的整数倍，但这显然是不可能的，因为√2不是整数，所以这个方程没有有理数解，因此√2是无理数。\\n\\n嗯，这些方法都证明了√2是无理数，不过我觉得最初的那个方法是最直接的，就是通过假设√2是有理数，然后推导出矛盾，进而证明它是无理数。这种方法逻辑上是严密的，所以我觉得这是最好的证明方式。\\n\\n总结一下，证明√2是无理数的步骤：\\n\\n1. 假设√2是一个有理数，即可以表示为两个互质整数a和b的比值，即√2 = a/b。\\n\\n2. 平方两边得到2 = a² / b²，即a² = 2b²。\\n\\n3. 由a² = 2b²可知，a必须是2b²的平方根，因此a必须是√2倍的b，即a = √2 * b。\\n\\n4. 但']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "58b8005e-a3df-43b4-bf07-6d189457276f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>请问如何证明根号2是无理数？或者有没有其他方法？\n",
      "\n",
      "嗯，我现在要证明根号2是无理数，对吧？或者有没有其他方法？首先，我记得证明√2是无理数的方法是通过假设它是有理数，然后推导出矛盾。让我先回忆一下这个过程。\n",
      "\n",
      "好，首先假设√2是一个有理数，也就是可以表示成两个整数的比值，也就是√2 = a/b，其中a和b都是正整数，并且b不等于0，而且这个分数已经是最简形式，也就是说a和b没有除了1以外的公约数。\n",
      "\n",
      "接下来，我可以把这个等式两边平方，得到2 = a² / b²，也就是a² = 2b²。这样的话，a²必须是2的倍数，因为右边是2b²，而b²是整数，所以a²也是整数，进而a也是整数。\n",
      "\n",
      "既然a²是2的倍数，那么a也必须是2的倍数，对吗？因为如果一个数的平方是偶数，那么这个数本身也必须是偶数。所以，a可以表示为2k，其中k是一个整数。\n",
      "\n",
      "现在，我把a代入原来的等式√2 = a/b，也就是√2 = (2k)/b。那么，两边平方得到2 = (4k²)/b²，进而得到b² = 2k²。\n",
      "\n",
      "同样地，这意味着b²也是2的倍数，所以b也必须是2的倍数。也就是说，b可以表示为2m，其中m是一个整数。\n",
      "\n",
      "现在，我已经有了a = 2k和b = 2m，代入√2 = a/b，也就是√2 = (2k)/(2m) = k/m。这样的话，√2 = k/m，其中k和m都是整数，而且k和m也是互质的，因为原来的a和b已经是最简形式。\n",
      "\n",
      "现在，这就导致了一个问题，因为如果√2是一个有理数，也就是可以表示成两个整数的比值，那么它必然是最简形式的，也就是k和m互质。但是，根据上面的推导，√2 = k/m，而k和m都必须是偶数吗？\n",
      "\n",
      "不对，等一下，k和m是否是偶数呢？因为a = 2k，所以k可以是任何整数，同样，b = 2m，所以m也是整数。现在，假设k和m都是偶数，那么我们可以继续分解下去，得到k = 2n，m = 2p，这样的话，√2 = (2n)/(2p) = n/p，这样就会导致√2 = n/p，其中n和p都是整数，这样就可以无限继续下去，这意味着√2可以无限分解下去，这显然是不对的，因为任何数都不能无限分解下去，除非它为0，而√2显然不等于0。\n",
      "\n",
      "因此，这说明我们的假设有问题，也就是说，如果√2是一个有理数，那么它就可以被表示成一个无限可分的比值，这显然是不可能的，所以我们的初始假设就是错误的，也就是说√2不是有理数，因此√2是无理数。\n",
      "\n",
      "或者，我可以换一种方式来思考。假设√2是有理数，那么它可以写成两个整数的比值a/b，其中a和b没有公因数。然后平方两边得到2 = a² / b²，所以a² = 2b²。这样的话，a必须是b的√2倍，也就是说，a必须是√2倍的b，而√2是无理数，这就意味着a也是无理数，这与我们的假设a是有理数矛盾，所以原来的假设是错误的，因此√2是无理数。\n",
      "\n",
      "另外，有没有其他方法来证明√2是无理数呢？我记得还有一种方法是通过代数的方式，假设√2是有理数，然后引入矛盾。或者，也可以使用二次方程的解的性质来证明。\n",
      "\n",
      "比如，考虑方程x² - 2 = 0，它的解是x = ±√2。如果这个方程有有理数解，那么根据有理根定理，解必须是±p/q，其中p和q是整数，而分母q必须是1，也就是说，p必须是±√2的整数倍，但这显然是不可能的，因为√2不是整数，所以这个方程没有有理数解，因此√2是无理数。\n",
      "\n",
      "嗯，这些方法都证明了√2是无理数，不过我觉得最初的那个方法是最直接的，就是通过假设√2是有理数，然后推导出矛盾，进而证明它是无理数。这种方法逻辑上是严密的，所以我觉得这是最好的证明方式。\n",
      "\n",
      "总结一下，证明√2是无理数的步骤：\n",
      "\n",
      "1. 假设√2是一个有理数，即可以表示为两个互质整数a和b的比值，即√2 = a/b。\n",
      "\n",
      "2. 平方两边得到2 = a² / b²，即a² = 2b²。\n",
      "\n",
      "3. 由a² = 2b²可知，a必须是2b²的平方根，因此a必须是√2倍的b，即a = √2 * b。\n",
      "\n",
      "4. 但\n"
     ]
    }
   ],
   "source": [
    "print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4f7d8b-ae44-4e9d-a386-5a9645456719",
   "metadata": {},
   "source": [
    "至此我们就完成了unsloth模型推理流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22e138d-3029-4e00-bd2a-c84017cee398",
   "metadata": {},
   "source": [
    "- 尝试使用unsloth调用Qwen模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a82a8c-69f8-4bea-8f30-ad4fd3c12c0d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;类似的，我们也可以使用unsloth调用Qwen蒸馏模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d2b1ccf8-7fc1-4eb1-bbb2-cb75b645f4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.151 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B does not have a padding token! Will use pad_token = <|vision_pad|>.\n"
     ]
    }
   ],
   "source": [
    "model_qwen, tokenizer_qwen = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"./model/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6ead11f1-fe6d-4de7-abf1-daa30539b357",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model_qwen) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1ee0518f-7b80-4ddf-a00f-d804da59cf64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>请问如何证明根号2是无理数？我知道有利用代数和几何的方法，但不知道具体怎么证明。\n",
      "\n",
      "嗯，证明根号2是无理数，这应该是数学中一个经典的证明例子吧。我记得以前老师上课的时候讲过，可能用了反证法。让我先回忆一下反证法的基本步骤：假设结论相反，然后推出矛盾，从而证明原命题成立。\n",
      "\n",
      "首先，根号2是一个无理数，也就是说，它不能表示成两个整数相除的形式。或者说，根号2的小数展开是无限不循环的。现在，我需要用反证法来证明这一点。\n",
      "\n",
      "那我们先假设相反的情况，也就是根号2是有理数。那么，根据有理数的定义，存在两个整数a和b，使得根号2 = a/b，其中a和b互质，也就是说它们没有公因数，除了1之外。\n",
      "\n",
      "接下来，我可以把等式两边平方一下，得到2 = a² / b²，然后两边同时乘以b²，得到2b² = a²。这一步应该是对的吧，两边乘以b²的话，左边是2乘以b²，右边是a²。\n",
      "\n",
      "接下来，我们可以得出a²是2的倍数，也就是说，a²是偶数。那么，a本身也是偶数，因为如果一个数的平方是偶数，那么这个数本身也必须是偶数。这个结论对吗？让我想想，比如，3是奇数，平方是9，也是奇数；2是偶数，平方是4，也是偶数。所以，如果a²是偶数，a一定也是偶数。对的，没错。\n",
      "\n",
      "既然a是偶数，那我们可以表示a为2k，其中k是某个整数。把a=2k代入原来的等式2b² = a²，得到2b² = (2k)² = 4k²。所以，2b² = 4k²，两边同时除以2，得到b² = 2k²。\n",
      "\n",
      "现在，我们发现b²也是偶数，那么b也必须是偶数，因为只有偶数的平方才是偶数。所以，b也是偶数。但是，这与我们最初的假设相矛盾，因为我们假设a和b是互质的，也就是它们之间没有公因数，但这里a和b都是偶数，说明它们都有一个公因数2，这和互质的假设相违背。\n",
      "\n",
      "所以，这个矛盾说明我们的假设是错误的，也就是说，根号2不可能是有理数，所以它一定是无理数。\n",
      "\n",
      "不过，我是不是哪里漏了什么？让我再检查一下每一步是否正确。首先，假设根号2是有理数，写成a/b，互质，对吗？是的，这没问题。然后平方得到2b² = a²，所以a²是偶数，进而a也是偶数，对吗？对的。然后设a=2k，代入得2b² =4k²，化简得b²=2k²，所以b²是偶数，b也是偶数。这样a和b都是偶数，说明它们有公因数2，和互质的假设矛盾。\n",
      "\n",
      "嗯，这样看来，整个证明过程应该是正确的。没有问题，对吗？对的，所以根号2确实是一个无理数。\n",
      "\n",
      "不过，我是不是还应该考虑其他可能性？比如，是否存在其他形式的a和b，或者我的步骤有没有什么地方可以优化？不过，我觉得这个证明已经很简洁了，步骤也很明确。不会有其他问题了。\n",
      "\n",
      "总结一下，我假设根号2是有理数，然后通过一系列的代数运算，导出了a和b都必须是偶数，这与它们互质的前提相矛盾，因此我的假设是错误的，根号2必须是无理数。这样看来，证明是正确的。\n",
      "\n",
      "**答案**\n",
      "\\boxed{\\sqrt{2}} 是无理数。\n",
      "</think>\n",
      "\n",
      "首先，我们假设根号2是有理数，即存在两个互质的整数a和b，使得\\(\\sqrt{2} = \\frac{a}{b}\\)。\n",
      "\n",
      "接下来，我们将等式两边平方，得到：\n",
      "\\[ 2 = \\frac{a^2}{b^2} \\]\n",
      "两边同时乘以\\(b^2\\)，得到：\n",
      "\\[ 2b^2 = a^2 \\]\n",
      "\n",
      "这说明\\(a^2\\)是2的倍数，因此a也是偶数。设\\(a = 2k\\)，其中k是整数。将a代入原式，得到：\n",
      "\\[ 2b^2 = (2k)^2 \\]\n",
      "\\[ 2b^2 = 4k^2 \\]\n",
      "两边同时除以2，得到：\n",
      "\\[ b^2 = 2k^2 \\]\n",
      "\n",
      "这说明\\(b^2\\)也是2的倍数，因此b也是偶数。这样，a和b都具有公因数2，这与它们互质的前提相矛盾。\n",
      "\n",
      "因此，我们的假设错误，根号2不能表示为两个整数的比值，即根号2是无理数。\n",
      "\n",
      "\\[\n",
      "\\boxed{\\sqrt{2}}\n",
      "\\] 是无理数。<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer_qwen([question], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model_qwen.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response = tokenizer_qwen.batch_decode(outputs)\n",
    "\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137554f3-0ccf-4965-be04-896b41cf4022",
   "metadata": {},
   "source": [
    "> 注，以下实验均以DeepSeek R1 LLama3蒸馏模型为例进行演示和讲解，若想替换为DeepSeek R1 Qwen模型，则可以直接替换模型名称即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea63eb8c-ace2-4287-b924-6d8783ad2c95",
   "metadata": {},
   "source": [
    "#### 2.带入问答模板进行回答"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10aee95-a38d-4e3b-9a81-3cf1050a1ecf",
   "metadata": {},
   "source": [
    "- 结构化输入方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "82b427d8-6931-406f-a2ab-38fdb38b613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_style_chat = \"\"\"请写出一个恰当的回答来完成当前对话任务。\n",
    "\n",
    "### Instruction:\n",
    "你是一名助人为乐的助手。\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c9fc959f-0190-4307-aa0b-43d26a81b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"你好，好久不见！\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f5d63aa1-48da-4173-87f0-12a334596f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['请写出一个恰当的回答来完成当前对话任务。\\n\\n### Instruction:\\n你是一名助人为乐的助手。\\n\\n### Question:\\n你好，好久不见！\\n\\n### Response:\\n<think>']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[prompt_style_chat.format(question, \"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f9fe11da-3433-4c80-8723-861cce6ee031",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_style_chat.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f45b4bf8-c61f-4276-bfed-86a28b34fb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1be62f68-2c81-4056-b678-948a5bc893f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c7459285-9821-4828-a26f-9c9c70b42420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<｜begin▁of▁sentence｜>请写出一个恰当的回答来完成当前对话任务。\\n\\n### Instruction:\\n你是一名助人为乐的助手。\\n\\n### Question:\\n你好，好久不见！\\n\\n### Response:\\n<think>\\n嗯，用户说“你好，好久不见！”，这可能是想表达一下想和我聊天，或者只是随便打个招呼。我的回应应该要友好又亲切，显示出乐于助人的态度。同时，要保持简洁，不让对话变得冗长。\\n\\n首先，回应开头可以用“你好呀！”这样比较随意又亲切。然后，接下来可以问用户最近怎么样，或者有什么想聊的，鼓励用户继续说下去。这样既回应了问候，又引导对话的发展。\\n\\n再想想，用户可能只是想和我打个招呼，或者想找些话题聊聊，所以回应要开放一些，给用户空间继续发言。比如，可以说最近有什么有趣的事情，或者有什么想聊的话题，这样用户可以根据自己的意愿回应。\\n\\n总的来说，我的回应应该包括以下几点：友好的问候、询问对方的情况、邀请对方分享更多内容，这样既符合助人为乐的态度，又能有效地引导对话继续下去。\\n</think>\\n\\n你好呀！很高兴见到你。最近怎么样？有什么想聊的吗？<｜end▁of▁sentence｜>']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "16ee5204-8c35-4e63-9e19-2a33894a4b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "嗯，用户说“你好，好久不见！”，这可能是想表达一下想和我聊天，或者只是随便打个招呼。我的回应应该要友好又亲切，显示出乐于助人的态度。同时，要保持简洁，不让对话变得冗长。\n",
      "\n",
      "首先，回应开头可以用“你好呀！”这样比较随意又亲切。然后，接下来可以问用户最近怎么样，或者有什么想聊的，鼓励用户继续说下去。这样既回应了问候，又引导对话的发展。\n",
      "\n",
      "再想想，用户可能只是想和我打个招呼，或者想找些话题聊聊，所以回应要开放一些，给用户空间继续发言。比如，可以说最近有什么有趣的事情，或者有什么想聊的话题，这样用户可以根据自己的意愿回应。\n",
      "\n",
      "总的来说，我的回应应该包括以下几点：友好的问候、询问对方的情况、邀请对方分享更多内容，这样既符合助人为乐的态度，又能有效地引导对话继续下去。\n",
      "</think>\n",
      "\n",
      "你好呀！很高兴见到你。最近怎么样？有什么想聊的吗？<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc39741-01df-4adb-88fb-885bc4340e2e",
   "metadata": {},
   "source": [
    "- 复杂问题测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1c89626c-b70a-42a5-8a91-c21c8b28cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"请证明根号2是无理数。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "62ad1369-ccd4-4885-872a-686277148c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_style_chat.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "53a02c58-9c03-4e22-bb38-d5f8eb13d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "723a82fc-2e5e-48c4-9460-1bf9d1bd2df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6c862fa3-ffe2-4736-829d-8f201716dddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "嗯，今天老师布置了一个证明√2是无理数的题目，我觉得挺有意思的，但也有点难。首先，我得先回忆一下什么是无理数。无理数就是不能表示成两个整数之比的数，也就是说，如果√2可以写成a/b，其中a和b都是整数，且b不等于0，那它就是有理数，否则就是无理数。\n",
      "\n",
      "那我先假设√2是有理数，也就是存在整数a和b，使得√2 = a/b，其中b不等于0。接下来，我可以把这个等式两边同时平方，得到2 = a²/b²。这意味着a² = 2b²。也就是说，a²是2的倍数，那么a也必须是2的倍数，因为如果一个数的平方是2的倍数，那这个数本身也必须是2的倍数。所以，我可以把a写成2k，其中k是一个整数。这样，a² = (2k)² = 4k²，代入上面的等式，得到4k² = 2b²，简化一下就是2k² = b²。同样地，这里b²是2的倍数，所以b也必须是2的倍数，同样可以表示为b = 2m，其中m是一个整数。这样，b² = (2m)² = 4m²，代入等式，得到2k² = 4m²，简化后是k² = 2m²。这样，又出现了k²是2的倍数，同样k也必须是2的倍数，所以k = 2n，n是一个整数。这样，k² = (2n)² = 4n²，代入等式，得到4n² = 2m²，简化后是2n² = m²。又一次，m²是2的倍数，所以m也必须是2的倍数，m = 2p，p是一个整数。这样，m² = (2p)² = 4p²，代入等式，得到2n² = 4p²，简化后是n² = 2p²。看起来这里出现了一个循环，每次都要求某个数是2的倍数，这意味着a和b都必须是无限多个2的因子，这是不可能的，因为a和b都是有限的整数。因此，这种假设导致了矛盾，所以√2不能表示成两个整数之比，因此√2是无理数。\n",
      "\n",
      "不过，我觉得可能哪里有问题，或者是不是还有其他方法来证明。比如，使用数学归纳法或者其他方法，但我觉得上面的方法已经足够了，因为它展示了如果√2是有理数，那么它必须是无限多个2的倍数，这是不可能的，所以结论是√2是无理数。\n",
      "</think>\n",
      "\n",
      "√2是无理数。证明如下：\n",
      "\n",
      "假设√2是有理数，则存在整数a和b（b≠0），使得√2 = a/b。两边平方得到2 = a²/b²，即a² = 2b²。因此，a必须是2的倍数，设a = 2k，k为整数。代入得(2k)² = 2b²，即4k² = 2b²，简化为2k² = b²。同样，b必须是2的倍数，设b = 2m，m为整数。代入得2k² = (2m)² = 4m²，简化为k² = 2m²。继续，k必须是2的倍数，设k = 2n，n为整数，代入得k² = 4n² = 2m²，简化为2n² = m²。同样，m必须是2的倍数，设m = 2p，p为整数，代入得m² = 4p² = 2n²，简化为2p² = n²。继续，n必须是2的倍数，设n = 2q，q为整数，代入得n² = 4q² = 2p²，简化为2q² = p²。显然，这种循环导致a和b必须含有无限多个因子2，这是不可能的，因为a和b是有限的整数。因此，假设错误，√2不能表示为两个整数之比，故√2是无理数。<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4258079b-67cd-43ef-8e5e-c8d20c6cc62f",
   "metadata": {},
   "source": [
    "#### 3.原始模型的医疗问题问答"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7dda0e-4a98-4ded-9be8-cc60ffb9f356",
   "metadata": {},
   "source": [
    "- 重新设置问答模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4dadf6c0-53c1-4d07-b0ec-57b723198f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "Please answer the following medical question. \n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153b915e-914e-41e4-b6be-d81ab77a7c8f",
   "metadata": {},
   "source": [
    "翻译如下：\n",
    "\n",
    "```python\n",
    "prompt_style = \"\"\"以下是一个任务说明，配有提供更多背景信息的输入。\n",
    "请写出一个恰当的回答来完成该任务。\n",
    "在回答之前，请仔细思考问题，并按步骤进行推理，确保回答逻辑清晰且准确。\n",
    "\n",
    "### Instruction:\n",
    "您是一位具有高级临床推理、诊断和治疗规划知识的医学专家。\n",
    "请回答以下医学问题。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea90929-b517-414c-8399-0a6f783b7c26",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们抽取部分medical-o1-reasoning-SFT数据集中问题进行提问，并查看初始状态下模型回答结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fee56999-513f-4a2b-9be5-677821c994f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e98f9e-6c5b-42f3-8e78-07951cdc069c",
   "metadata": {},
   "source": [
    "翻译：一位61岁的女性，有长期在咳嗽或打喷嚏等活动中发生不自主尿液流失的病史，但夜间没有漏尿。她接受了妇科检查和Q-tip测试。根据这些检查结果，膀胱测量（cystometry）最可能会显示她的残余尿量和逼尿肌收缩情况如何？\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "96bffeb4-8252-4e4e-8c3e-9a3375ab940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_2 = \"Given a patient who experiences sudden-onset chest pain radiating to the neck and left arm, with a past medical history of hypercholesterolemia and coronary artery disease, elevated troponin I levels, and tachycardia, what is the most likely coronary artery involved based on this presentation?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6623b554-cae5-4a9f-b033-9c356ae031ad",
   "metadata": {},
   "source": [
    "翻译：面对一位突发胸痛并放射至颈部和左臂的患者，其既往病史包括高胆固醇血症和冠状动脉疾病，同时伴有升高的肌钙蛋白I水平和心动过速，根据这些临床表现，最可能受累的冠状动脉是哪一条？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e929f7-614c-4f86-9cd0-b6417c1adbd1",
   "metadata": {},
   "source": [
    "- 问答测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3c2290e9-1e9e-4fef-bde8-cc84591437bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = tokenizer([prompt_style.format(question_1, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "outputs1 = model.generate(\n",
    "    input_ids=inputs1.input_ids,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response1 = tokenizer.batch_decode(outputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "02e6c870-23e6-4f6f-a990-720dfc36887b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, so I have this medical case to analyze. Let me break it down step by step. The patient is a 61-year-old woman with a history of involuntary urine loss when she coughs or sneezes, but she doesn't leak at night. She's had a gynecological exam and a Q-tip test. I need to figure out what cystometry would show regarding her residual volume and detrusor contractions.\n",
      "\n",
      "First, the involuntary urine loss during activities like coughing or sneezing makes me think of stress urinary incontinence. That's usually due to urethral sphincter deficiency, especially since she's a woman. The fact that she doesn't leak at night suggests that her bladder capacity is good because she can hold it until she gets to a toilet, which is typical in stress incontinence.\n",
      "\n",
      "Next, the gynecological exam and Q-tip test. I'm a bit rusty on the Q-tip test. I think it's used to assess urethral function. The Q-tip is a small catheter or probe that's inserted into the urethra to measure resistance. If the resistance is high, it might indicate sphincter dysfunction. So, if the Q-tip test shows increased resistance, that supports the diagnosis of stress incontinence.\n",
      "\n",
      "Now, thinking about cystometry. Cystometry is a test where a catheter is placed in the bladder, and the patient is asked to void. The test measures bladder capacity, compliance, and detrusor contractions. It's also used to assess how the bladder responds to filling.\n",
      "\n",
      "Residual volume refers to the volume of urine left in the bladder after voiding. If the patient has stress incontinence, I'd expect her residual volume to be low because she doesn't retain much urine. High residual volume is more typical in cases of bladder overactivity, like urgency incontinence, where the bladder doesn't empty completely.\n",
      "\n",
      "Detrusor contractions are the contractions of the detrusor muscle, which help push urine out. In normal cases, these contractions are involuntary and are triggered by the need to void. If the detrusor is overactive, these contractions can happen too frequently or without filling, causing urgency.\n",
      "\n",
      "In this patient, since she has stress incontinence, I would expect the cystometry to show low residual volume because she doesn't hold much urine. As for detrusor contractions, stress incontinence is more about the sphincter not closing, not so much the detrusor muscle overactivity. So, the detrusor contractions might be normal, not hyperactive.\n",
      "\n",
      "Wait, but sometimes in stress incontinence, there can be some detrusor activity, but it's usually not the primary issue. The main issue is the sphincter. So, the detrusor contractions would be within normal limits, not excessive.\n",
      "\n",
      "Putting it all together, cystometry would likely show a low residual volume and normal detrusor contractions. This would support the diagnosis of stress urinary incontinence rather than something like overactive bladder where detrusor contractions would be excessive.\n",
      "</think>\n",
      "\n",
      "Cystometry in this 61-year-old woman with a history of stress urinary incontinence would most likely reveal a **low residual volume** and **normal detrusor contractions**. \n",
      "\n",
      "### Explanation:\n",
      "- **Low Residual Volume:** This aligns with her history of stress incontinence, where she doesn't retain urine, leading to involuntary leakage during activities like coughing.\n",
      "- **Normal Detrusor Contractions:** Stress incontinence is primarily associated with urethral sphincter deficiency rather than detrusor overactivity, which would cause excessive contractions. Therefore, the detrusor contractions would be within normal limits.\n",
      "\n",
      "This finding supports the diagnosis of stress urinary incontinence rather than conditions like overactive bladder, where detrusor contractions would be excessive.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "print(response1[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9addf3d-8181-4fbb-808f-27f1dd65f274",
   "metadata": {},
   "source": [
    "翻译如下：\n",
    "\n",
    "\\<think>  \n",
    "好的，我正在尝试分析这个医学问题。我们一步步来分解。患者是一位61岁的女性，有在咳嗽或打喷嚏等活动中发生不自主尿液流失的病史，但她夜间没有漏尿。她正在接受妇科检查和Q-tip测试。问题是关于膀胱测量（cystometry）会显示她的残余尿量和逼尿肌收缩情况。\n",
    "\n",
    "首先，我知道在像打喷嚏或咳嗽等活动中发生不自主尿液流失通常与压力性尿失禁有关。压力性尿失禁通常发生在尿道肌肉不足以在压力增大的情况下（比如咳嗽时）防止膀胱漏尿时。\n",
    "\n",
    "接下来是Q-tip测试。根据我记得的，Q-tip是一种用于测量尿道压力曲线的尿道导管。它通常用于评估尿道功能。Q-tip测试阳性结果，即在Valsalva操作过程中尿道压力低于膀胱内压，与内源性括约肌缺陷相关，这是一种压力性尿失禁类型。\n",
    "\n",
    "由于患者有在活动中出现不自主漏尿的病史，但夜间没有漏尿，更可能是压力性尿失禁，而不是像急迫性尿失禁那样的情况，急迫性尿失禁通常伴有夜间漏尿。因此，如果Q-tip测试阳性，提示内源性括约肌缺陷。\n",
    "\n",
    "现在，谈到膀胱测量。膀胱测量是一种测试，旨在测量膀胱在充盈过程中的反应以及逼尿肌的收缩情况。它可以显示是否存在膀胱过度活动症（OAB），即引起急迫感和频尿的情况，或是否存在逼尿肌低活动性，导致尿潴留。\n",
    "\n",
    "在这种情况下，患者的主要问题是压力性尿失禁，这更与无法在压力增大时保持尿液有关。膀胱测量会查看逼尿肌的收缩情况。如果逼尿肌低活动性，它将不能强有力地收缩以排空膀胱，导致残余尿量。但如果逼尿肌过度活跃，可能会收缩过度，导致急迫感。\n",
    "\n",
    "鉴于患者有压力性尿失禁的病史和Q-tip测试阳性，提示内源性括约肌缺陷，我认为膀胱测量会显示逼尿肌的收缩是正常的。问题不在于逼尿肌收缩的能力，而是无法密封尿道以保持压力。因此，残余尿量可能是正常的，除非有明显的尿潴留，但关键发现是逼尿肌的收缩是正常的，而不是过度活跃。\n",
    "\n",
    "等等，但会不会有残余尿量？如果患者排尿后膀胱中残留一些尿液，那就是残余尿量。但如果没有尿潴留的症状，比如膀胱饱胀或排尿困难，那么这种情况的可能性较小。主要问题是在活动中发生的尿失禁，因此逼尿肌收缩是正常的，残余尿量在正常范围内，除非有其他情况。\n",
    "\n",
    "所以，综合来看，膀胱测量可能会显示逼尿肌的收缩正常，残余尿量正常。问题更多是在括约肌方面，而不是逼尿肌。  \n",
    "\\</think>\n",
    "\n",
    "根据对患者病史和Q-tip测试结果的分析，膀胱测量最可能显示逼尿肌的收缩正常，残余尿量正常。主要问题似乎是由于内源性括约肌缺陷引起的压力性尿失禁，如Q-tip测试阳性所示。这种情况通常影响尿道括约肌在压力增大时防止漏尿的能力，而不是逼尿肌的收缩能力。因此，逼尿肌的收缩并未过度活跃，残余尿量在正常范围内。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bf7ec2-fcd2-4b19-91c0-0eae77f42ee4",
   "metadata": {},
   "source": [
    "标准答案：\n",
    "\n",
    "在这种压力性尿失禁的情况下，膀胱测压检查（cystometry）最可能显示**正常的排尿后残余尿量**，因为压力性尿失禁通常不会影响膀胱排空功能。此外，由于压力性尿失禁主要与**身体用力**有关，而不是膀胱过度活动症（OAB），因此在测试过程中**不太可能观察到逼尿肌的非自主收缩**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a2c20cbb-56a9-471a-94a5-b1f605bc3973",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs2 = tokenizer([prompt_style.format(question_2, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "outputs2 = model.generate(\n",
    "    input_ids=inputs2.input_ids,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response2 = tokenizer.batch_decode(outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "786cca4c-988b-4369-b9e5-56688f1ff643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, so I have a patient presenting with sudden-onset chest pain that's radiating to the neck and left arm. Hmm, that makes me think of a cardiac issue. The past medical history includes hypercholesterolemia and coronary artery disease, which are both risk factors for heart problems. Also, elevated troponin I levels are a big indicator of myocardial injury, probably from a heart attack. Tachycardia could be a response to that injury or maybe a separate issue.\n",
      "\n",
      "I should consider which coronary artery is most likely involved. The pain radiating to the left arm makes me think of the left anterior descending (LAD) artery, but that's just one possibility. The right coronary artery (RCA) could also be involved, especially since the pain might radiate down the right arm as well. The left circumflex artery is another option, but the typical presentation might point more towards LAD or RCA.\n",
      "\n",
      "Troponin I elevation is more specific to the left ventricle, so if the LAD is involved, that might be the case. However, without more specific information like ECG changes or imaging, it's hard to be certain. I think the best approach is to mention both RCA and LAD as possibilities based on the typical radiations. Maybe the patient's specific presentation could help narrow it down further.\n",
      "\n",
      "I should also consider other possibilities, like a left main or right main coronary artery, but those are less common. The key here is to not get too specific without more data. So, in conclusion, both LAD and RCA are possible, but the exact artery can't be definitively determined without further evaluation.\n",
      "</think>\n",
      "\n",
      "The patient's presentation of sudden-onset chest pain radiating to the neck and left arm, coupled with elevated troponin I levels and tachycardia, suggests a cardiac origin. While the left anterior descending (LAD) artery is a common culprit due to its distribution, the right coronary artery (RCA) is also a potential candidate, especially if the pain radiates to the right arm. Without additional information such as ECG changes or imaging, both LAD and RCA remain possibilities. Further evaluation would be necessary to determine the exact artery involved. \n",
      "\n",
      "**Answer:** The most likely coronary artery involved is either the right coronary artery (RCA) or the left anterior descending (LAD) artery, depending on the specific presentation and further diagnostic evaluation.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "print(response2[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f13f64-11bc-47bb-8994-e1ccea729bb9",
   "metadata": {},
   "source": [
    "翻译如下：\n",
    "\n",
    "\\<think>  \n",
    "好的，我有一位患者，突然出现胸痛，并放射到颈部和左臂。这让我想到了心脏病发作，因为这些症状很经典——心绞痛或心肌梗死。左臂痛、颈部痛，有时还会伴随下颌或背部的疼痛，这些都可能与冠状动脉问题相关。\n",
    "\n",
    "从病史来看，患者有**高胆固醇血症**（即高胆固醇）和**冠状动脉疾病**，这两个因素都是**动脉粥样硬化**的风险因素，可能导致冠状动脉发生堵塞。**肌钙蛋白I升高**是一个很大的线索，因为肌钙蛋白是心肌受损时释放的心脏酶，通常表明发生了心肌梗死。另外，患者还出现了**心动过速**，即心跳比平常快。在心肌梗死时，心脏可能会加速跳动，以试图通过增加心脏输出量来补偿被阻塞的冠状动脉。\n",
    "\n",
    "考虑到冠状动脉，**左主冠状动脉**（LMCA）为整个左侧心脏提供血液，包括左心室，而左心室是一个关键的泵血肌肉。如果这里发生堵塞，可能导致更严重的心肌梗死，因为左心室至关重要。**右冠状动脉**为右心室和左心室下壁提供血液，这里的堵塞也是可能的，但**左主冠状动脉**通常与上述症状更相关，尤其是当肌钙蛋白升高时。\n",
    "\n",
    "所以，将所有因素综合考虑，最可能受累的冠状动脉是**左主冠状动脉**（LMCA）。患者的病史、肌钙蛋白升高以及典型的胸痛放射症状都指向了这一动脉作为罪魁祸首。\n",
    "\n",
    "\\</think>\n",
    "\n",
    "最可能受累的冠状动脉是**左主冠状动脉（LMCA）**。\n",
    "\n",
    "**解释：**\n",
    "- **症状：** 患者突发胸痛并放射至颈部和左臂，以及肌钙蛋白升高，提示急性冠状动脉综合症，可能是心肌梗死。\n",
    "- **病史：** 高胆固醇血症和冠状动脉疾病病史是动脉粥样硬化的风险因素，可能导致冠状动脉堵塞。\n",
    "- **心动过速：** 心率增加可能是心脏为补偿心肌血流减少而产生的反应。\n",
    "- **冠状动脉考虑：** 左主冠状动脉供应左心室，这个肌肉对心脏功能至关重要。与右冠状动脉相比，左主冠状动脉的堵塞会导致更严重且危及生命的心肌梗死，右冠状动脉通常供应的是不那么关键的区域。\n",
    "\n",
    "因此，症状、肌钙蛋白升高以及患者的病史强烈指向**左主冠状动脉**（LMCA）作为最可能的罪魁祸首。<｜end▁of▁sentence｜>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5acf498-88f5-47b7-831e-2dcbcb96e589",
   "metadata": {},
   "source": [
    "标准答案：\n",
    "\n",
    "根据患者表现出的突然胸痛并放射至颈部和左臂，结合其有高胆固醇血症和冠状动脉疾病的病史，肌钙蛋白升高和心动过速，临床症状强烈提示左前降支（LAD）动脉受累。该动脉通常是引发此类症状的罪魁祸首，因为它供应了心脏的大部分区域。放射性疼痛和肌钙蛋白升高的组合表明心肌受损，这使得LAD成为最可能的致病动脉。然而，在没有进一步的诊断检查（如心电图）的情况下，最终的确诊仍需等待确认。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7cdfee-2b25-4fdc-9ecd-ac5ea4a30e41",
   "metadata": {},
   "source": [
    "能够看出，在原始状态下，模型能够进行推理并给出回复，但实际上第一个回答过程并不符合医学规范，而第二个问题则直接回答错误。由此可见，在初始状态下，模型对于medical-o1-reasoning-SFT数据集问答效果并不好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2dc95-c4c8-4a03-acb9-b8c5bce92e9d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来尝试进行微调，并测试微调后模型问答效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e5d96-c090-4d06-b133-c36b789e41b8",
   "metadata": {},
   "source": [
    "### 二、最小可行性实验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6842985-e49e-44f3-9fa5-2e2ba3ec4a3b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们尝试进行模型微调，对于当前数据集而言，我们可以带入原始数据集的部分数据进行微调，也可以带入全部数据并遍历多次进行微调。对于大多数的微调实验，我们都可以从最小可行性实验入手进行微调，也就是先尝试带入少量数据进行微调，并观测微调效果。若微调可以顺利执行，并能够获得微调效果，再考虑带入更多的数据进行更大规模微调。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6672971b-0d59-4c5a-9461-fa7e4c8e1cb4",
   "metadata": {},
   "source": [
    "#### 1.数据集准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441dc132-747d-40ec-9269-f315678b29e0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里我们直接从huggingface上下载medical-o1-reasoning-SFT数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0b548-4e41-406b-8d52-110509156eb9",
   "metadata": {},
   "source": [
    "- 设置代理环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ac067-8d97-4999-9a3e-1ef305751f39",
   "metadata": {},
   "source": [
    "&emsp;&emsp;由于huggingface网络受限，下载数据集前需要先进行网络环境设置。若是AutoDL服务器，则可以按照如下方式开启学术加速，从而顺利连接huggingface并进行数据集下载："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "281d6b32-4b56-4184-80da-202d7dd589af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baef1f84-74f4-4779-9326-a11502aa6ae1",
   "metadata": {},
   "source": [
    "- 下载数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d4f6f-38ce-438e-be64-951f4b907fd7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来使用datasets进行数据集下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5924a688-b61e-4d6d-bc2e-fa0b8c013ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.internal-mirrors.ucloud.cn/simple\n",
      "Requirement already satisfied: datasets in /home/ubuntu/miniconda3/lib/python3.12/site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (24.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fd4635bd-5d5d-406b-900d-eccbfaf261a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90685ac-6510-4fd4-9c2d-def17e0dc6d5",
   "metadata": {},
   "source": [
    "再次确认提示词模板："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "acef18a3-d669-4acf-8da7-4da22b3aaee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "Please answer the following medical question. \n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53db946-57c9-40c0-aded-1ac2462f12ec",
   "metadata": {},
   "source": [
    "然后提取并设置文本生成结束的标记："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "05c75cc9-61f4-4ff3-9124-34ff78456a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜end▁of▁sentence｜>'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0117c824-4486-4b17-bbd0-cb3ff9a34f4d",
   "metadata": {},
   "source": [
    "然后定义函数，用于对medical-o1-reasoning-SFT数据集进行修改，Complex_CoT列和Response列进行拼接，并加上文本结束标记："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9dee28cc-1fe8-4c79-860b-c7792b269530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Question\"]\n",
    "    cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    texts = []\n",
    "    for input, cot, output in zip(inputs, cots, outputs):\n",
    "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b85065-f6a9-4bff-ae58-cdd940bb2cbf",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250206180316919.png\" alt=\"image-20250206180316919\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4456066-89d8-4ad0-9a2c-edf72eca77f9",
   "metadata": {},
   "source": [
    "在最小可行性实验中，我们可以只下载500条数据进行微调即可看出效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c18961a1-0db6-404f-81ad-182b2b62a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train[0:500]\",trust_remote_code=True)\n",
    "\n",
    "dataset = load_dataset(\"./dataset/FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train[0:500]\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "85106a25-688e-447f-b411-11cc1b0206df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Question': 'A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?',\n",
       " 'Complex_CoT': \"Okay, let's think about this step by step. There's a 61-year-old woman here who's been dealing with involuntary urine leakages whenever she's doing something that ups her abdominal pressure like coughing or sneezing. This sounds a lot like stress urinary incontinence to me. Now, it's interesting that she doesn't have any issues at night; she isn't experiencing leakage while sleeping. This likely means her bladder's ability to hold urine is fine when she isn't under physical stress. Hmm, that's a clue that we're dealing with something related to pressure rather than a bladder muscle problem. \\n\\nThe fact that she underwent a Q-tip test is intriguing too. This test is usually done to assess urethral mobility. In stress incontinence, a Q-tip might move significantly, showing urethral hypermobility. This kind of movement often means there's a weakness in the support structures that should help keep the urethra closed during increases in abdominal pressure. So, that's aligning well with stress incontinence.\\n\\nNow, let's think about what would happen during cystometry. Since stress incontinence isn't usually about sudden bladder contractions, I wouldn't expect to see involuntary detrusor contractions during this test. Her bladder isn't spasming or anything; it's more about the support structure failing under stress. Plus, she likely empties her bladder completely because stress incontinence doesn't typically involve incomplete emptying. So, her residual volume should be pretty normal. \\n\\nAll in all, it seems like if they do a cystometry on her, it will likely show a normal residual volume and no involuntary contractions. Yup, I think that makes sense given her symptoms and the typical presentations of stress urinary incontinence.\",\n",
       " 'Response': 'Cystometry in this case of stress urinary incontinence would most likely reveal a normal post-void residual volume, as stress incontinence typically does not involve issues with bladder emptying. Additionally, since stress urinary incontinence is primarily related to physical exertion and not an overactive bladder, you would not expect to see any involuntary detrusor contractions during the test.'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca9256-067a-4f7b-a339-e0473bc4a81e",
   "metadata": {},
   "source": [
    "然后进行结构化处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "82c249c5-ec25-448d-8045-46042d9ad963",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fc62f8-96f2-4905-b0e6-c28a60954ce7",
   "metadata": {},
   "source": [
    "将数据集整理为如下形式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1f34f952-656e-44d6-a778-8a70690d4c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n\\n### Question:\\nA 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\\n\\n### Response:\\n<think>\\nOkay, let's think about this step by step. There's a 61-year-old woman here who's been dealing with involuntary urine leakages whenever she's doing something that ups her abdominal pressure like coughing or sneezing. This sounds a lot like stress urinary incontinence to me. Now, it's interesting that she doesn't have any issues at night; she isn't experiencing leakage while sleeping. This likely means her bladder's ability to hold urine is fine when she isn't under physical stress. Hmm, that's a clue that we're dealing with something related to pressure rather than a bladder muscle problem. \\n\\nThe fact that she underwent a Q-tip test is intriguing too. This test is usually done to assess urethral mobility. In stress incontinence, a Q-tip might move significantly, showing urethral hypermobility. This kind of movement often means there's a weakness in the support structures that should help keep the urethra closed during increases in abdominal pressure. So, that's aligning well with stress incontinence.\\n\\nNow, let's think about what would happen during cystometry. Since stress incontinence isn't usually about sudden bladder contractions, I wouldn't expect to see involuntary detrusor contractions during this test. Her bladder isn't spasming or anything; it's more about the support structure failing under stress. Plus, she likely empties her bladder completely because stress incontinence doesn't typically involve incomplete emptying. So, her residual volume should be pretty normal. \\n\\nAll in all, it seems like if they do a cystometry on her, it will likely show a normal residual volume and no involuntary contractions. Yup, I think that makes sense given her symptoms and the typical presentations of stress urinary incontinence.\\n</think>\\nCystometry in this case of stress urinary incontinence would most likely reveal a normal post-void residual volume, as stress incontinence typically does not involve issues with bladder emptying. Additionally, since stress urinary incontinence is primarily related to physical exertion and not an overactive bladder, you would not expect to see any involuntary detrusor contractions during the test.<｜end▁of▁sentence｜>\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c478885f-5a4b-431c-b705-6faea5bcf087",
   "metadata": {},
   "source": [
    "- 数据集保存地址"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304ca9f9-34ed-45d8-be58-b4ed829dbcbf",
   "metadata": {},
   "source": [
    "默认情况下数据集保存在主目录下.cache文件夹中，数据文件格式如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e6e91-be9b-4fb7-90ba-7aac308bf293",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250206195216257.png\" alt=\"image-20250206195216257\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614a4418-2f7b-4b3e-9efa-efa2a5fd6dd7",
   "metadata": {},
   "source": [
    "#### 2.开启微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b7956b-039b-483f-9b50-15caf056dfb9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后即可把模型设置为微调模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "800ccbbd-c119-49f0-82b2-5485fde1b64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e02e0ce4-8a9c-4137-92da-e75f1245035a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 增加的\n",
    "FastLanguageModel.for_training(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "028f9bf9-c089-4546-af8e-9eb56ca31b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  \n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  \n",
    "    bias=\"none\",  \n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  \n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d5b06-50b4-4d78-a902-e2df87221f3f",
   "metadata": {},
   "source": [
    "然后导入相关的库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6d135473-1c60-4555-9a14-8abd5e5cabbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e168c5-df66-4a26-b65d-ad5251db7e76",
   "metadata": {},
   "source": [
    "创建有监督微调对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1e1622f1-a0d9-495e-8162-8623027d4a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting train dataset to ChatML (num_proc=2): 100%|██████████| 500/500 [00:00<00:00, 2169.69 examples/s]\n",
      "Applying chat template to train dataset (num_proc=2): 100%|██████████| 500/500 [00:01<00:00, 374.73 examples/s]\n",
      "Tokenizing train dataset (num_proc=2): 100%|██████████| 500/500 [00:01<00:00, 297.39 examples/s]\n",
      "Tokenizing train dataset (num_proc=2): 100%|██████████| 500/500 [00:00<00:00, 1393.32 examples/s]\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n",
    "        warmup_steps=5,\n",
    "        max_steps=60,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4836f8b5-b608-4af2-be5c-c1a4d4de74c5",
   "metadata": {},
   "source": [
    "这段代码主要是用 **`SFTTrainer`** 进行 **监督微调（Supervised Fine-Tuning, SFT）**，适用于 `transformers` 和 `Unsloth` 生态中的模型微调：\n",
    "**1. 导入相关库**\n",
    "- **`SFTTrainer`**（来自 `trl` 库）：  \n",
    "  - `trl`（Transformer Reinforcement Learning）是 Hugging Face 旗下的 `trl` 库，提供 **监督微调（SFT）** 和 **强化学习（RLHF）** 相关的功能。\n",
    "  - `SFTTrainer` 主要用于 **有监督微调（Supervised Fine-Tuning）**，适用于 `LoRA` 等低秩适配微调方式。\n",
    "\n",
    "- **`TrainingArguments`**（来自 `transformers` 库）：  \n",
    "  - 这个类用于定义 **训练超参数**，比如批量大小、学习率、优化器、训练步数等。\n",
    "\n",
    "- **`is_bfloat16_supported()`**（来自 `unsloth`）：  \n",
    "  - 这个函数检查 **当前 GPU 是否支持 `bfloat16`（BF16）**，如果支持，则返回 `True`，否则返回 `False`。\n",
    "  - `bfloat16` 是一种更高效的数值格式，在 **新款 NVIDIA A100/H100** 等 GPU 上表现更优。\n",
    "\n",
    "**2. 初始化 `SFTTrainer` 进行模型微调**\n",
    "\n",
    "##### **参数解析**\n",
    "##### **① `SFTTrainer` 部分**\n",
    "| 参数 | 作用 |\n",
    "|------|------|\n",
    "| `model=model` | 指定需要进行微调的 **预训练模型** |\n",
    "| `tokenizer=tokenizer` | 指定 **分词器**，用于处理文本数据 |\n",
    "| `train_dataset=dataset` | 传入 **训练数据集** |\n",
    "| `dataset_text_field=\"text\"` | 指定数据集中哪一列包含 **训练文本**（在 `formatting_prompts_func` 里处理） |\n",
    "| `max_seq_length=max_seq_length` | **最大序列长度**，控制输入文本的最大 Token 数量 |\n",
    "| `dataset_num_proc=2` | **数据加载的并行进程数**，提高数据预处理效率 |\n",
    "\n",
    "##### **② `TrainingArguments` 部分**\n",
    "| 参数 | 作用 |\n",
    "|------|------|\n",
    "| `per_device_train_batch_size=2` | 每个 **GPU/设备** 的训练批量大小（较小值适合大模型） |\n",
    "| `gradient_accumulation_steps=4` | **梯度累积步数**（相当于 `batch_size=2 × 4 = 8`） |\n",
    "| `warmup_steps=5` | **预热步数**（初始阶段学习率较低，然后逐步升高） |\n",
    "| `max_steps=60` | **最大训练步数**（控制训练的总步数，此处总共约消耗60*8=480条数据） |\n",
    "| `learning_rate=2e-4` | **学习率**（`2e-4` = 0.0002，控制权重更新幅度） |\n",
    "| `fp16=not is_bfloat16_supported()` | 如果 **GPU 不支持 `bfloat16`，则使用 `fp16`（16位浮点数）** |\n",
    "| `bf16=is_bfloat16_supported()` | 如果 **GPU 支持 `bfloat16`，则启用 `bfloat16`（训练更稳定）** |\n",
    "| `logging_steps=10` | **每 10 步记录一次训练日志** |\n",
    "| `optim=\"adamw_8bit\"` | **使用 `adamw_8bit`（8-bit AdamW优化器）减少显存占用** |\n",
    "| `weight_decay=0.01` | **权重衰减（L2 正则化）**，防止过拟合 |\n",
    "| `lr_scheduler_type=\"linear\"` | **学习率调度策略**（线性衰减） |\n",
    "| `seed=3407` | **随机种子**（保证实验结果可复现） |\n",
    "| `output_dir=\"outputs\"` | **训练结果的输出目录** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153cc26d-f6c9-4979-bb12-9c07849bcf06",
   "metadata": {},
   "source": [
    "然后设置wandb（可选）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "536a41b7-50bf-4f0f-8240-777b3c80f84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.internal-mirrors.ucloud.cn/simple\n",
      "Collecting wandb\n",
      "  Downloading https://pypi.internal-mirrors.ucloud.cn/packages/7d/53/361846bf44dcf3bc5a4be0e0cb662b42c7e6996d71c903c936e191657e0d/wandb-0.19.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from wandb) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Using cached https://pypi.internal-mirrors.ucloud.cn/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading https://pypi.internal-mirrors.ucloud.cn/packages/1d/9a/4114a9057db2f1462d5c8f8390ab7383925fe1ac012eaa42402ad65c2963/GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: platformdirs in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from wandb) (6.0.0)\n",
      "Collecting pydantic<3,>=2.6 (from wandb)\n",
      "  Using cached https://pypi.internal-mirrors.ucloud.cn/packages/f4/3c/8cc1cc84deffa6e25d2d0c688ebb80635dfdbf1dbea3e30c541c8cf4d860/pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from wandb) (2.32.2)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading https://pypi.internal-mirrors.ucloud.cn/packages/12/7f/0e4459173e9671ba5f75a48dda2442bcc48a12c79e54e5789381c8c6a9bc/sentry_sdk-2.22.0-py2.py3-none-any.whl (325 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.8/325.8 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting setproctitle (from wandb)\n",
      "  Downloading https://pypi.internal-mirrors.ucloud.cn/packages/52/79/78b05c7d792c9167b917acdab1773b1ff73b016560f45d8155be2baa1a82/setproctitle-1.3.5-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from wandb) (69.5.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading https://pypi.internal-mirrors.ucloud.cn/packages/a0/61/5c78b91c3143ed5c14207f463aecfc8f9dbb5092fb2869baf37c273b2705/gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0 (from pydantic<3,>=2.6->wandb)\n",
      "  Using cached https://pypi.internal-mirrors.ucloud.cn/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=2.6->wandb)\n",
      "  Downloading https://pypi.internal-mirrors.ucloud.cn/packages/8d/f0/49129b27c43396581a635d8710dae54a791b17dfc50c70164866bbf865e3/pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.12.2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading https://pypi.internal-mirrors.ucloud.cn/packages/04/be/d09147ad1ec7934636ad912901c5fd7667e1c858e19d355237db0d0cd5e4/smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, pydantic-core, docker-pycreds, annotated-types, pydantic, gitdb, gitpython, wandb\n",
      "Successfully installed annotated-types-0.7.0 docker-pycreds-0.4.0 gitdb-4.0.12 gitpython-3.1.44 pydantic-2.10.6 pydantic-core-2.27.2 sentry-sdk-2.22.0 setproctitle-1.3.5 smmap-5.0.2 wandb-0.19.7\n"
     ]
    }
   ],
   "source": [
    "! pip install wandb\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0fda0b4a-d2ca-4b36-87ca-59d025ef7a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwww6v\u001b[0m (\u001b[33mwww6v-vip\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"1a8d27529016174442b34dd0ae898bfa88d2156c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bea7966-4da4-463e-b4c4-cf628b74f3d2",
   "metadata": {},
   "source": [
    "然后开始微调："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cd782462-3d29-490c-9d63-4cb43a240883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 500 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 02:25, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.941600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.471300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.410600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.320400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.355500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.322600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1278f09-4a99-4833-900a-659cd3e002eb",
   "metadata": {},
   "source": [
    "此时wandb中显示内容如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a107a62e-eed6-49ad-8c65-7b51b12819ba",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250206200441907.png\" alt=\"image-20250206200441907\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "05245a7d-6239-436d-847d-5d3188e1ec52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=1.4703421751658121, metrics={'train_runtime': 157.2751, 'train_samples_per_second': 3.052, 'train_steps_per_second': 0.381, 'total_flos': 1.8014312853602304e+16, 'train_loss': 1.4703421751658121})"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fa75b2-0345-4c18-8eda-ebf1241b269b",
   "metadata": {},
   "source": [
    "注意，unsloth在微调结束后，会自动更新模型权重（在缓存中），因此无需手动合并模型权重即可直接调用微调后的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "efaf4cf4-ef13-4e3f-bc7f-d08fd36ffc4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "64bca6ee-fc4c-4ca0-ab9d-d099f628f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_style.format(question_1, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5bbb2f57-8e83-484f-b8b6-e34d9d9a529f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, let's see. We have a 61-year-old woman with a history of involuntary urine loss, especially during things like coughing or sneezing. That sounds like a classic presentation of urge incontinence. Now, when we think about what's happening here, it's likely that her bladder is contracting involuntarily. This could be due to an overactive bladder, which is often caused by an underlying condition like a neurological issue or a problem with the pelvic floor muscles.\n",
      "\n",
      "Now, let's talk about the Q-tip test. This test is designed to check how much urine is left in the bladder after it's emptied. If it's positive, it means there's residual urine, which can contribute to incontinence because it increases the chances of accidental leakage. So, if the Q-tip test is positive, it's pretty clear that her bladder isn't emptying completely.\n",
      "\n",
      "Then there's the cystometry part. This test is used to measure how much urine remains in the bladder after it's emptied. If there's residual volume, it suggests that the bladder isn't emptying properly, which can be a big problem for someone with urge incontinence. Also, during this test, we look at how the bladder muscle behaves. If it's showing involuntary contractions, it supports the idea that there's an overactive bladder.\n",
      "\n",
      "So, putting it all together, we're expecting to see some residual volume in the bladder because the Q-tip test came back positive. This makes sense because residual urine can lead to further leakage. And, if there's an overactive bladder, we'll see involuntary contractions during the cystometry. These findings would all fit together to explain why she's experiencing these symptoms. It looks like everything points to a need for treatment that addresses both the residual urine and the overactive nature of her bladder.\n",
      "</think>\n",
      "Based on the findings of the Q-tip test and the clinical presentation of the 61-year-old woman, cystometry would most likely reveal that her bladder has residual volume. This is because the Q-tip test detected residual urine, indicating that her bladder does not empty completely. Additionally, cystometry would likely show evidence of involuntary detrusor contractions, which is consistent with the symptoms of urge incontinence and an overactive bladder.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46783905-93f7-4886-9920-2ca46d836593",
   "metadata": {},
   "source": [
    "测试第二个问题问答效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6b850a79-eba4-44fc-98d8-1488a85742a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_style.format(question_2, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c1834b90-c3e2-4a0b-b967-0293726a78a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, so we've got this patient here experiencing sudden chest pain that's spreading to their neck and left arm. That's a classic symptom of a heart attack. And they've got a history of high cholesterol and heart disease, which is a red flag for someone at risk for a cardiac event. Plus, their troponin levels are elevated, which directly points to cardiac damage, and they're tachycardic, meaning their heart is racing. These all line up with what we'd expect from a coronary artery blockage.\n",
      "\n",
      "Now, we need to figure out which artery is most likely the culprit here. Coronary artery disease usually affects the left main artery, left anterior descending artery, or the right coronary artery. Given that the pain is radiating to the neck and left arm, it's telling us that the left main artery is probably the one involved. This artery is connected to several branches that supply the left side of the heart, which explains the widespread pain.\n",
      "\n",
      "So, putting it all together, the most likely artery causing the issue is the left main coronary artery. That's the artery that's most directly linked to the symptoms described and fits the pattern of a heart attack that's affecting the left side of the heart.\n",
      "</think>\n",
      "The most likely coronary artery involved in this presentation is the left main coronary artery. This artery is the primary supplier of blood to the left side of the heart, including the left anterior descending artery and the circumflex artery, which are both involved in supplying the left ventricle. The radiating pain to the neck and left arm is indicative of a blockage in this artery, as it is connected to several branches that supply the left side of the heart, aligning with the symptoms described.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb726b1-1e9a-45e3-a197-992e67e1e5b3",
   "metadata": {},
   "source": [
    "> 此时模型认为“左主冠状动脉最可能是该患者症状的罪魁祸首”，但实际上应该是“左前降支（LAD）动脉受累”导致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d7c4e0-d1cd-4fc9-8dbd-c76ed581f2b3",
   "metadata": {},
   "source": [
    "能够发现，第一个问题回答更加规范，并且回答正确。但第二个问题仍然回答错误。由此可以考虑继续进行大规模微调。不过在此之前，我们可以将现在小规模微调的模型进行本地保存。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83871526-b408-4090-8ea5-73ed0a7daba7",
   "metadata": {},
   "source": [
    "#### 3.模型合并"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3b846d-ebe6-4b9a-a5ff-e5971258e3fc",
   "metadata": {},
   "source": [
    "此时本地保存的模型权重在`outputs`文件夹中："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378c031-80f1-4eac-9775-7557188981ab",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250206195427494.png\" alt=\"image-20250206195427494\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e9f53-0ee6-4830-86b9-4ba2f04c5169",
   "metadata": {},
   "source": [
    "然后可使用如下代码进行模型权重合并："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "15877f4c-e0c4-4ef0-ba89-70381b2d1436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 70.71 out of 125.72 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 67.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "new_model_local = \"DeepSeek-R1-Medical-COT-Tiny\"\n",
    "model.save_pretrained(new_model_local) \n",
    "tokenizer.save_pretrained(new_model_local)\n",
    "\n",
    "model.save_pretrained_merged(new_model_local, tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ac9133-227e-477e-9e0e-1b26c2c87480",
   "metadata": {},
   "source": [
    "保存结束后，即可在当前文件夹中看到对应模型："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dbba3b-9956-42b7-9247-953f2240f640",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250206200056475.png\" alt=\"image-20250206200056475\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743ee184-b03c-40c5-bfc8-be63e134bfbf",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250206200157110.png\" alt=\"image-20250206200157110\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f62c52e-387b-4673-b214-ab3e060a76fb",
   "metadata": {},
   "source": [
    "然后即可将其推送到huggingface上并保存为GGUF格式文件并进行调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b37e85dd-7926-4716-a046-ac07ca67d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained_gguf(\"dir\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "# model.save_pretrained_gguf(\"dir\", tokenizer, quantization_method = \"q8_0\")\n",
    "# model.save_pretrained_gguf(\"dir\", tokenizer, quantization_method = \"f16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e7bc26-8d7a-4388-aa4b-4c48b6875f21",
   "metadata": {},
   "source": [
    "### 三、完整高效微调实验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7446a0c5-c8cf-45ae-a069-8a4ee40f9e60",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们尝试带入全部数据进行高效微调，以提升模型微调效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6b634ea8-b9a1-4b80-adec-8ca219f862b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "Please answer the following medical question. \n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b173a851-3466-4596-903a-3e591408e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Question\"]\n",
    "    cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    texts = []\n",
    "    for input, cot, output in zip(inputs, cots, outputs):\n",
    "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e5cdcf-be2a-4c69-96b7-66490e6029e1",
   "metadata": {},
   "source": [
    "此时读取全部数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fd3d9b24-29e0-4868-afb4-527ae245d67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25371/25371 [00:00<00:00, 37925.20 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n\\n### Question:\\nA 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\\n\\n### Response:\\n<think>\\nOkay, let's think about this step by step. There's a 61-year-old woman here who's been dealing with involuntary urine leakages whenever she's doing something that ups her abdominal pressure like coughing or sneezing. This sounds a lot like stress urinary incontinence to me. Now, it's interesting that she doesn't have any issues at night; she isn't experiencing leakage while sleeping. This likely means her bladder's ability to hold urine is fine when she isn't under physical stress. Hmm, that's a clue that we're dealing with something related to pressure rather than a bladder muscle problem. \\n\\nThe fact that she underwent a Q-tip test is intriguing too. This test is usually done to assess urethral mobility. In stress incontinence, a Q-tip might move significantly, showing urethral hypermobility. This kind of movement often means there's a weakness in the support structures that should help keep the urethra closed during increases in abdominal pressure. So, that's aligning well with stress incontinence.\\n\\nNow, let's think about what would happen during cystometry. Since stress incontinence isn't usually about sudden bladder contractions, I wouldn't expect to see involuntary detrusor contractions during this test. Her bladder isn't spasming or anything; it's more about the support structure failing under stress. Plus, she likely empties her bladder completely because stress incontinence doesn't typically involve incomplete emptying. So, her residual volume should be pretty normal. \\n\\nAll in all, it seems like if they do a cystometry on her, it will likely show a normal residual volume and no involuntary contractions. Yup, I think that makes sense given her symptoms and the typical presentations of stress urinary incontinence.\\n</think>\\nCystometry in this case of stress urinary incontinence would most likely reveal a normal post-void residual volume, as stress incontinence typically does not involve issues with bladder emptying. Additionally, since stress urinary incontinence is primarily related to physical exertion and not an overactive bladder, you would not expect to see any involuntary detrusor contractions during the test.<｜end▁of▁sentence｜>\""
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"./dataset/FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train\",trust_remote_code=True)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d4eb8209-4f35-4551-8a8b-35aecb0eccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  \n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  \n",
    "    bias=\"none\",  \n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  \n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7dce0-84cd-46e2-b0a7-a05e3b81b211",
   "metadata": {},
   "source": [
    "这里设置epoch为3，遍历3次数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4739bb1a-0d51-48d2-beb3-0d6449e7ad55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting train dataset to ChatML (num_proc=2): 100%|██████████| 25371/25371 [00:00<00:00, 32660.41 examples/s]\n",
      "Applying chat template to train dataset (num_proc=2): 100%|██████████| 25371/25371 [00:01<00:00, 13687.14 examples/s]\n",
      "Tokenizing train dataset (num_proc=2): 100%|██████████| 25371/25371 [00:23<00:00, 1057.69 examples/s]\n",
      "Tokenizing train dataset (num_proc=2): 100%|██████████| 25371/25371 [00:09<00:00, 2714.73 examples/s]\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs = 1,   # 3 \n",
    "        warmup_steps=5,\n",
    "        # max_steps=60,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ba8576d5-8fb1-4b5d-8707-24703ff79945",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 25,371 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 3,171\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/jupyterfile/wei/wandb/run-20250223_162013-0mjocghx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/www6v-vip/huggingface/runs/0mjocghx' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/www6v-vip/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/www6v-vip/huggingface' target=\"_blank\">https://wandb.ai/www6v-vip/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/www6v-vip/huggingface/runs/0mjocghx' target=\"_blank\">https://wandb.ai/www6v-vip/huggingface/runs/0mjocghx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3171' max='3171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3171/3171 2:03:46, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.333900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.317900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.287300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.313600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.330800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.296900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.292200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.291200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.242200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.235300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.289900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.226500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.301600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.187800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.301300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.248800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.236500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.229600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.281300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.293400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.243900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.259700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.242200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.251300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.256200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.239900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.222500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.329100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.288500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.195100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.249400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.248200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.200300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.214200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.239400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.212400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.213900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.189600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.230900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.255700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.261900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.258500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.283100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.295500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.204500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.245100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.217400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.245500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.285500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.247800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.209100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.224500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.217900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.231100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.231500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.246800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.254600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>1.213100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.211900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>1.221300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>1.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.268200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.209700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.245600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.232100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>1.236700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.239100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.178600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.254500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.223800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.243200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>1.179500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.237100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>1.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>1.158500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.230900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>1.241200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.240100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>1.196300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.213400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>1.147700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.178300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>1.192200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.209500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>1.173800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.252400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.174600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.232700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>1.232800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.168200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>1.189100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.221200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>1.236300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.218900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>1.212800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.231400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>1.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>1.209200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.204100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>1.206500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>1.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>1.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>1.219900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>1.237900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>1.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>1.197800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>1.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.222500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>1.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.198100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>1.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>1.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.188700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.216700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>1.227200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>1.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>1.261200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.186000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>1.213200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>1.168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>1.183700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>1.189300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.218800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>1.203500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>1.229600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>1.176200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>1.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>1.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>1.181900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>1.160400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>1.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>1.177900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>1.244300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>1.182700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>1.152200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.216600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>1.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>1.169900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>1.166500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>1.205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.243200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>1.193900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>1.180300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>1.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>1.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.216600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>1.166700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>1.176600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>1.181200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>1.167700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.178200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>1.168200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>1.193800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>1.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>1.179700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.179100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>1.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>1.184300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>1.162200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>1.220900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>1.200400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>1.183600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>1.233500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>1.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.198900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>1.174700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>1.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>1.196600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>1.193400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>1.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>1.195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>1.188300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>1.168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.155900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>1.143700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>1.196800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>1.182600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>1.192900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>1.197100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>1.185800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>1.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>1.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.224400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>1.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>1.208400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>1.189100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>1.206700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.174200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>1.233100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>1.168200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>1.195900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>1.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>1.181900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>1.209500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>1.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>1.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.160900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>1.194900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>1.212900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>1.150200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>1.162600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>1.223200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>1.205900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>1.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>1.205200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.190700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>1.140300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2370</td>\n",
       "      <td>1.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>1.199800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2390</td>\n",
       "      <td>1.135600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.132900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2410</td>\n",
       "      <td>1.178600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>1.124600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2430</td>\n",
       "      <td>1.204100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>1.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.151500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>1.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2470</td>\n",
       "      <td>1.179500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>1.192100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2490</td>\n",
       "      <td>1.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.151400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2510</td>\n",
       "      <td>1.152900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>1.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2530</td>\n",
       "      <td>1.139100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>1.154300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>1.191700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>1.209200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2570</td>\n",
       "      <td>1.175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>1.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2590</td>\n",
       "      <td>1.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.154400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2610</td>\n",
       "      <td>1.155500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>1.197800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2630</td>\n",
       "      <td>1.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>1.198400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>1.181800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>1.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2670</td>\n",
       "      <td>1.184900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>1.184600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2690</td>\n",
       "      <td>1.207700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2710</td>\n",
       "      <td>1.202700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>1.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2730</td>\n",
       "      <td>1.216400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>1.160800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>1.159700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>1.187100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2770</td>\n",
       "      <td>1.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>1.195700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2790</td>\n",
       "      <td>1.194200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2810</td>\n",
       "      <td>1.177400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>1.194600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2830</td>\n",
       "      <td>1.183200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>1.208100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>1.215700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>1.160600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2870</td>\n",
       "      <td>1.171400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>1.140200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2890</td>\n",
       "      <td>1.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.197200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2910</td>\n",
       "      <td>1.185300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>1.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2930</td>\n",
       "      <td>1.178200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>1.166900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.185800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>1.152700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2970</td>\n",
       "      <td>1.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>1.182600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2990</td>\n",
       "      <td>1.186400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.143800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3010</td>\n",
       "      <td>1.196500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>1.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3030</td>\n",
       "      <td>1.163500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>1.182900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>1.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>1.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3070</td>\n",
       "      <td>1.168300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>1.134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3090</td>\n",
       "      <td>1.160700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3110</td>\n",
       "      <td>1.142100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>1.139900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3130</td>\n",
       "      <td>1.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>1.229200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>1.189800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>1.149600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3170</td>\n",
       "      <td>1.122800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca87f0b5-c6cc-4885-b19d-bc3b3cd1cfae",
   "metadata": {},
   "source": [
    "这里总共训练约2个小时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0ff611dd-e6be-4066-97d4-b288711465bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3171, training_loss=1.205654080641153, metrics={'train_runtime': 7429.134, 'train_samples_per_second': 3.415, 'train_steps_per_second': 0.427, 'total_flos': 9.317611621994988e+17, 'train_loss': 1.205654080641153})"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57354364-e766-4094-b3f2-03d9f736110f",
   "metadata": {},
   "source": [
    "带入两个问题进行测试，均有较好的回答效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f7893ec0-c557-4347-91f0-af1a28163451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Alright, let's think about this. We have a 61-year-old woman who's been dealing with involuntary urine leakage whenever she coughs or sneezes. That sounds like stress incontinence. You know, when there's pressure on the bladder, like from coughing, it makes sense she'd leak.\n",
      "\n",
      "Now, stress incontinence is all about the urethra, which is the narrow tube that carries urine from the bladder. The urethra doesn't quite close up tightly enough, so when there's pressure, like during a cough, it just lets a bit of urine slip out.\n",
      "\n",
      "Okay, so what about the Q-tip test? This test is used to check how well the urethra closes. If it doesn't close properly, that would explain the stress incontinence. It's like the urethra isn't strong enough to hold up against pressure.\n",
      "\n",
      "Thinking about what cystometry might show, it's used to see how the bladder behaves. We're interested in two things: residual volume, which is the amount of urine left in the bladder after it empties, and detrusor contractions, which are those muscle contractions that push urine out.\n",
      "\n",
      "In stress incontinence, the bladder is usually empty, so the residual volume should be low. The detrusor contractions, though, are usually normal because the issue isn't with the bladder's ability to contract but rather the urethra's ability to hold the urine in.\n",
      "\n",
      "So, putting it all together, I'd expect to see a low residual volume in cystometry, because the bladder doesn't hold onto much urine when it's supposed to be empty. And normal detrusor contractions, because the bladder itself isn't the problem here—it's the urethra.\n",
      "\n",
      "Yeah, that makes sense. Stress incontinence is more about the urethra not closing properly, not the bladder contracting. So, everything checks out with what we know about this condition.\n",
      "</think>\n",
      "In the case of stress incontinence, such as the involuntary urine leakage during activities like coughing or sneezing that you've described, cystometry would most likely reveal a low residual volume. This is because the bladder is typically empty during these episodes, as the problem lies with the urethra's ability to close properly under pressure, rather than the bladder's ability to hold urine. Additionally, the detrusor contractions would be normal, as the issue isn't with the bladder's contraction but rather the urethra's closure. Therefore, you would expect to see a low residual volume and normal detrusor contractions in the cystometry findings.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "question = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\n",
    "\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f8bba1c2-a7c7-4e82-9795-48a117c70c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, let's think about this. We've got a patient with sudden chest pain that's shooting down to the neck and arm. That's pretty classic for a heart attack, right? The pain is usually a big clue here. And this patient also has a history of high cholesterol and coronary artery disease. That's like putting a big red flag up for potential heart issues.\n",
      "\n",
      "Now, let's see what else we've got. The elevated troponin levels are another huge red flag. Troponin is like the body's signal that something's really wrong with the heart muscle. And then there's the tachycardia, which means the heart is beating really fast. All these things together scream acute coronary syndrome, especially a myocardial infarction.\n",
      "\n",
      "So, what's happening here? It's probably that a blockage in one of the coronary arteries is causing all this trouble. Now, which artery could it be? Hmm, there's a pattern here. The pain radiates to the left arm and neck, which is usually a hint that we're dealing with the left anterior descending artery, or LAD. That's the main artery supplying the front of the heart, and if it's blocked, it can cause all this pain.\n",
      "\n",
      "Oh, and let's not forget that this patient had hypercholesterolemia and coronary artery disease. Those are risk factors that can lead to atherosclerosis, which is basically a buildup of plaque in the arteries. Plaque can rupture and cause a blood clot, leading to a heart attack. The LAD is notorious for being a common site for these kinds of blockages.\n",
      "\n",
      "So, putting all this together, it really looks like the left anterior descending artery is the culprit here. It's the most likely site of the blockage causing the patient's symptoms. Yeah, that makes sense, given everything we know. It's pretty consistent with what we'd expect from a heart attack presentation like this.\n",
      "</think>\n",
      "The most likely coronary artery involved in this presentation is the left anterior descending (LAD) artery. The sudden onset of chest pain radiating to the left arm and neck, along with the patient's history of hypercholesterolemia and coronary artery disease, strongly suggests an acute myocardial infarction. The LAD artery is a common site for blockages due to atherosclerosis, which can cause these symptoms. The elevated troponin levels and tachycardia further support this diagnosis of a heart attack involving the LAD.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "question = \"Given a patient who experiences sudden-onset chest pain radiating to the neck and left arm, with a past medical history of hypercholesterolemia and coronary artery disease, elevated troponin I levels, and tachycardia, what is the most likely coronary artery involved based on this presentation?\"\n",
    "\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b0182-51de-4e71-a15a-003d56c1da7f",
   "metadata": {},
   "source": [
    "最后进行模型权重保存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "099775b8-7ff0-4dee-b422-a72bc1d5a559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 69.46 out of 125.72 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 69.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "new_model_local = \"DeepSeek-R1-Medical-COT\"\n",
    "model.save_pretrained(new_model_local) \n",
    "tokenizer.save_pretrained(new_model_local)\n",
    "\n",
    "model.save_pretrained_merged(new_model_local, tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c0330c-06ae-4acf-895e-24d2ab6e9c9d",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250206201842044.png\" alt=\"image-20250206201842044\" style=\"zoom:50%;\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
